{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "Jackson",
	"language": "en",
	"home_page_url": "https://jackson.gd/",
	"feed_url": "https://jackson.gd/feed/feed.json",
	"description": "Hello, I&#39;m Jackson.",
	"author": {
		"name": "Jackson",
		"url": "https://jackson.gd/me/"
	},
	"items": [
		{
			"id": "https://jackson.gd/blog/2024-05-01-cacophony/",
			"url": "https://jackson.gd/blog/2024-05-01-cacophony/",
			"title": "Sound Pedro 2024: Cacophony",
			"content_html": "<h1 id=\"sound-pedro-2024\" tabindex=\"-1\">Sound Pedro 2024 <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2024-05-01-cacophony/#sound-pedro-2024\">#</a></h1>\n<p>This year I presented a sonic installation at Sound Pedro alongside a cohort of other artists presenting wacky and experimental audio works. Initially, I thought it would just make sense to write up a technical overview of my piece but the whole experience and very existence of Sound Pedro made me think it would be interesting to tell a story alongside the technical details.</p>\n<h2 id=\"a-little-context\" tabindex=\"-1\">A little context <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2024-05-01-cacophony/#a-little-context\">#</a></h2>\n<p>In 2021, prior to moving to Long Beach, I was first introduced to the cast of wacky sound artists that regularly filled San Pedro each year. For those unacquainted with the south coast of LA, San Pedro is a sleepy blue-collar town perched right off the massive twin ports of LA and Long Beach. I would have never ended up in that town had an up-end for a co-worker of a friend who liked to &quot;get a shit-ton of speakers and wire up an array&quot;. Now that's my kind of stuff.</p>\n<figure><video src=\"https://jackson.gd/vid/tape.mp4\" width=\"100%\" controls=\"\">Your browser does not support the video tag.</video><figcaption>Playing the sound of tape, unspooled between chairs and concrete</figcaption></figure>\n<p>That's my time at Oslo at school for an esoteric degree in music technology. I had always wanted to put up my own sound insulation. The idea of occupying some permanent sonic space that people might pass through and observe thought was genuinely so cool. When I saw dozens and dozens of weirdos scattered around the Angels Gate Cultural Center I was beaming.</p>\n<p>My installation was actually a much older project spurred at an &quot;a-ha&quot; kind of observation during a sonification class at the University of Oslo. Part of one of our projects involved capturing and repurposing <a href=\"https://en.wikipedia.org/wiki/Found_object_%28music%29\">&quot;found sounds&quot;</a> from YouTube (yes, I know that there are many more appropriate places). This led me to become quite familiar with the Swiss Army Knife of Media, FFmpeg. In some rabbit hole, I found it had the little-known option to pull a partial clip from a video at any time marker. My mind raced. Could you pull exclusively audio? What was the size of a 30-second clip? 300 kilobytes. Could I download clips 10 at once - was there throttling? For whatever reason, Google appears to be quite lenient with YouTube's access.</p>\n<p>Cacophony was a small project that took a random list of URLs of YouTube links and would fade in and out clips of audio from these videos. What made the sounds being heard all the more interesting was that they were scraped. They were scraped and direct relationship to YouTube's recommendation system. I wrote a simple crawler to start the home page and click random links, collect YouTube URLs, and then repeat. Hearing this traversal becomes a lot more interesting with context in mind.</p>\n<p>All the code is open-source - <a href=\"https://github.com/jacksongoode/cacophony/\">check it out here</a>. I plan to make a more technical write up with some details on the entire system. In the meantime, you can read the very ostentatious description I submitted.</p>\n<blockquote>\n<p>Cacophony is an immersive sound installation that attempts to capture the inundation of modern media hosting platforms through their traversal. Sites like YouTube, TikTok, Instagram, among others, provide users with virtually boundless and aggressively curated content. Though thought of as visual platforms, sound holds arguably equal weight in the luring of users to their hosted content. Through the programmatic collection and extraction of media, sounds once bound by the frame of moving images are played out of place and time. This siphoning of audio from these clips occurs in real time and is hemmed by restrictions both from the network and media host. Simultaneously, thumbnails from the videos are retrieved, projected and refracted across the local surroundings, further emphasizing the blurred boundaries of sound. “Cacophony” serves disjointed, aural moments from the navigation of these platforms to reflect on how they vie for our attention with often overwhelming consequences.</p>\n</blockquote>\n<p>For the installation, I wanted to go as big as I could under my strict budget of $60. After some extensive Facebook marketplace sleuthing, I found a five-channel home speaker setup with an amplifier. I spent a few more hours than I should, reprograming the script and getting everything to work in a five-channel array (all programmed in Python - yes, you heard that's right. An a-sync real-time multi-channel installation in Python - clearly the best language for this use-case).</p>\n<figure><video src=\"https://jackson.gd/vid/pad.mp4\" width=\"100%\" controls=\"\">Your browser does not support the video tag.</video><figcaption>My spot</figcaption></figure>\n<p>The hours preparing before Sound Pedro were just as frantic as it was the first time arriving on that defunct naval base. I spent the past night dusting off speakers, cutting wire, frantically coding and adding catches for robustness and so the speakers wouldn't accidentally explode. Yet, there was very little at home to give an impression of the concrete cave I would be in.</p>\n<p>I was happy when my friends quietly appeared in the stream of visitors.</p>\n<figure><img src=\"https://jackson.gd/vid/surprise.jpg\" width=\"100%\" alt=\"Me, surprised\"><figcaption>Me, surprised</figcaption></figure>\n<h2 id=\"after-the-noise\" tabindex=\"-1\">After the noise <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2024-05-01-cacophony/#after-the-noise\">#</a></h2>\n<p>Like with a lot of art projects, you spend far more time invested, perfecting, tuning, scrutinizing than anyone will ever see. When I came down to setting up the space, I decided to bring my whole living room up and into whatever space I would be given. Lamps, lights, chairs, my table. Mine turned out to be one of the coziest concrete bungalows there.</p>\n<p>With some continued routing woes, I wasn't able to project the colorful display of video thumbnails as I intended, so I used my laptop's display instead. Folks passing by were excited, confused, disturbed, entertained, and generally interested, since there is very little visual indication of all of the many simultaneous streams of audio that were booming off the semi-circular apartment I was squatting in for the evening.</p>\n<figure><video src=\"https://jackson.gd/vid/ongoing.mp4\" width=\"100%\" controls=\"\">Your browser does not support the video tag.</video><figcaption>A quick clip - sadly the drones of the surrounding performances did just that :)</figcaption></figure>\n",
			"date_published": "2024-05-01T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2023-07-19-montreal/",
			"url": "https://jackson.gd/blog/2023-07-19-montreal/",
			"title": "Montreal",
			"content_html": "<p>I didn't think  I would end up in Montreal. I think my only awareness of the city was a unit on Quebec within my French class in middle/highschool. Even then, the recollections I have are of maple syrup dripped over snow, poutine, bitter winters and ice sculptures.</p>\n<p>However, it came more into focus during the last years of undergrad where thoughts towards the future, my continued education within Cognitive Science, were looming. McGill had a department and program in neuroscience which I had mentioned to my fellow cog-sci classmates thinking it would be a cool program abroad (though Canada in my mind felt far less international than anywhere else.) And I've been considering moving from Long Beach, and Montreal was always a place on my list.</p>\n<p>While I ended up not pursuing down that road, remarkably, one of those close friends I mentioned this program to did! She is now close to finishing a PhD in Neuroscience studying the effects of canabanoids during prenatal development in mice. I was able to visit her during the warmest period in Montreal (flight likely discounted by the raging wildfires) and so became quite enamoured with a beautiful été in Montreal.</p>\n<h2 id=\"french\" tabindex=\"-1\">French <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2023-07-19-montreal/#french\">#</a></h2>\n<p>I knew there was a French influence historically (and presumably more linguistically now) but I had no idea the range to which French somewhat dominates the city (and provence) in a largely Anglophone part of the world. I was told (paraphrasing now by word of mouth) of the historical and economical (class) divide between the English speaking population and French and the historical disparity of capital and political representation that has come to represent. I was given a short history lesson from my friend's partner's mother who joined us one lunch. She has migrated from Durham, North Carolina (of all places!) and had a strong sense of Montreal's history and culture.</p>\n<p>The wealthy English population took the spoils and political power during the spoils of <a href=\"https://en.wikipedia.org/wiki/Battle_of_the_Plains_of_Abraham\">the war</a> and largely assimilated with the rest of English speaking Canada, leaving Francophones as a kind of second-class citizen. The power flipped in the 70s with the French party finally taking seats and representing Quebec and along with it, legislation to preserve French as the first language spoken in Quebec. This means oddly specific sign laws that require French being on and often above English whenever a public sign is made and requirements that any new students migrating to Quebec be taught in French-language schools (the only exception if both parents when to English speaking schools in Quebec).</p>\n<h2 id=\"today\" tabindex=\"-1\">Today <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2023-07-19-montreal/#today\">#</a></h2>\n<p>But this admirable push to preserve their historical uniqueness has also resulted in some unusual consequences. One is a practical isolation from opportunities within the rest of Canada and the US. Those learning French alone will inevitably struggle with the English world that surrounds them. The second is the much more unique cultural and political shifts that appeared more progressive than typically found in North America. This could be felt even whilst roaming the streets.</p>\n<p>I imagine Montreal, as the seat of Quebec, will move forward on a little bit of it's own axis. Meaning, it's feels while Montreal is already quite an international city, it is a city that is actively fighting <em>for</em> a more linguistically diverse population. This is in contrast to the US where we are just starting to acknowledge that instructing children in English, who's native language or home language, is to exclude a richness of diversity (and all that entails) into the US for the sake of assimilation into a world that reflects the past. Because of these efforts to revitalize and reintegrate French on a prominently non-French side of the world (both North and South), Montreal will have to contend and fight for a kind of right to a cultural and linguistic independence (not sure if there's enough momentum to succeed but it's on the minds of the radicals!)</p>\n<h2 id=\"sights\" tabindex=\"-1\">Sights <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2023-07-19-montreal/#sights\">#</a></h2>\n<p>I walk quite a lot. I'm heavily inspired by other walkers like <a href=\"https://walkingtheworld.substack.com/\">Chris Arnade</a> and <a href=\"https://pedestrian.substack.com/\">Alex Wolfe</a>. These are people who are incredibly gifted at journaling around the worlds they walk in. For me at least, walking is a unique way of knowing the world you're in that tangibly connects you with the places and people that exist there. This experience (which I might blog on some later time) reminds me of Baudelaire's flâneur, an aloof sort of person who saunters through urban modernity with a philosophical eye.</p>\n<p>The architecture is this mix of sort of French European and American or maybe what would be like sort of North American, Canadian But the policies of the US have clear impacts, especially the commercial and the businesses that reside within the US have definitely taken a strong foothold in Canada as well. It makes sense given the geographical proximity.</p>\n<p>I think biking, walkability, public transportation here are all very European in flavor. Likely due to early involvement of the French in city planning and transit systems. I mean the French were huge developers of train systems in Taiwan and Vietnam, I mean that has like colonial history but it makes sense why Montreal and maybe Quebec generally has a lot of really good well thought-out public transit.</p>\n<h3 id=\"tipping\" tabindex=\"-1\">Tipping <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2023-07-19-montreal/#tipping\">#</a></h3>\n<p>One odd thing is tipping culture. Why is it here? I would suspect or I would have suspected that such a huge complaint of like French tourists visiting the US and from chatting with like Europeans generally is that the tipping culture in the US is kind of a landmark folly that really represents more than just some nostalgic relic of the &quot;friendly American culture&quot; (read white, middle-class, suburbia), it represents clear exploitation in the service industry, a failure of the working class to rally around fair compensation, and how culture wars so easily distract us from class struggle.</p>\n<p>Most Europeans think the whole concept is ridiculous and I think a lot of French tourists, many of whom packed the hostels, would think the same. I'm sure it's leaked over from the US and might represent a much more similar service industry to the US than Europe.</p>\n<h3 id=\"cafes\" tabindex=\"-1\">Cafés <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2023-07-19-montreal/#cafes\">#</a></h3>\n<p>Given that, café culture is huge and the cafés are better compared to the US. It seems like people care more about the accessibility and quality of a place it get a cup of coffee in the neighborhood. Likely due to the poor city planning in most American cities, the neighborhood café has been replaced with a suburban sprawl and lacks the charming nooks found in older European cities since no one wants to walk an hour out of your cul-de-sac to get to a café.</p>\n<h3 id=\"buildings\" tabindex=\"-1\">Buildings <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2023-07-19-montreal/#buildings\">#</a></h3>\n<p>The city also does a good job of creating a lot of of beautiful and seemingly affordable public housing. It reminds me a lot of Oslo where a lot of apartments were built thoughtfully. They're placed in good locations and it seems like most folks get a much bigger window or a balcony. Again, these are likely nicer areas, but even in denser neighborhoods reminiscent of New York or Brooklyn, there seems to be enough open space and yards for people to make their own. That also applies to the skyline as well.</p>\n<p>And it's also felt that Montreal has built alongside history, something not so common of North America (most history has been eradicated physically and culturally from Turtle Island). Montreal is lucky in the sense that it was settled in 1642 and has seen a bunch of growth after a slow period in the 80's and 90's. A lot of the old brick, industrial aesthetic blends within new constructions and renovations. Even when the brick looks new, it maintains a modern yet industrial vibe that fits with the surrounding buildings. There is near <em>constant</em> construction and repairs of the roads. It's a fun challenge navigating by bike in the city.</p>\n<h3 id=\"coming-back\" tabindex=\"-1\">Coming back <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2023-07-19-montreal/#coming-back\">#</a></h3>\n<p>I'm excited to return to Montreal next year (for a wedding), if only it remained as beautiful and temperate in the winter!</p>\n",
			"date_published": "2023-08-01T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2023-06-03-thought/",
			"url": "https://jackson.gd/blog/2023-06-03-thought/",
			"title": "Thinking out Loud",
			"content_html": "<h1 id=\"publishing-thoughts\" tabindex=\"-1\">Publishing Thoughts <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2023-06-03-thought/#publishing-thoughts\">#</a></h1>\n<p>I've been wanting an excuse to publicize more stuff.</p>\n<p>A big barrier to making material public is of course the packaging. Sure an idea or a project might be cool, but you're never able to communicate this information in the same way that you might in person in conversation. It's always felt to me that if you are in full control of the platform - like on a personal website or blog - the ideas your publish ought to be at least a little polished.</p>\n<p>I recently watched a video from a YouTuber that creates long-form essays about contemporary cultural phenomenon. He had made a second channel just for fun that published little small anecdotes (<a href=\"https://www.youtube.com/watch?v=XWVHGHRUrss\">here's one</a>).</p>\n<p>It echos a general feeling around what we consider as &quot;work&quot; or &quot;productive&quot; or &quot;skilled&quot; that I've been trying to get a pulse on. How the things we do, once we have an expected level of quality from our personal work, become far less rewarding, far less exploratory, and often result in more conservative choices we make along the way.</p>\n<p>I'll hopefully write a lot more like this, shorter blerbs, littered with mistakes, that might turn into longer thoughts and constructions. Maybe it will serve as a public journal or something with an audience in mind. As long as it's out here!</p>\n",
			"date_published": "2023-06-03T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2022-08-24-film-for-music-2/",
			"url": "https://jackson.gd/blog/2022-08-24-film-for-music-2/",
			"title": "Music for a Series of Scenes: Part 2",
			"content_html": "<p>Almost two years later, I've continued to contribute to this growing catalog of &quot;music videos&quot; and thought it might be time to jump back in with more discussion of a select few films, the music I chose for them and the nuance in-between. Unintentionally, I've continued to watch, enjoy and incorporate films without color. Maybe it's the nostalgia for a time unknown, or just that there are a lot of good movies spread over the years. Since the series has been steadily growing, I think it's about time to discuss another two with the theme of color in mind.</p>\n<h2 id=\"la-jetee\" tabindex=\"-1\">La Jetée <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2022-08-24-film-for-music-2/#la-jetee\">#</a></h2>\n<p>Chris Marker is a real treasure. Though I have many of his films on my watchlist, the few I've seen have placed him in a category of his own. Often filled with simple scenes, transformed through narration, or, in his later works, digital manipulations, Marker poses questions unanswerable and opens boxes hidden under a lens or in plain sight. There's a mysterious fan, whose Marker related videos have been floating around YouTube that <a href=\"https://www.youtube.com/watch?v=ekenS1hRwoU\">might be making a documentary</a>. It ought to be interesting if/when it comes out.</p>\n<p>La Jetée is just an incredible film. Some point fun at it as <a href=\"https://letterboxd.com/film/la-jetee/\">&quot;the best powerpoint in the world&quot;</a>, of course for the project's most striking decision: the absence of motion. It's for this reason even Marker describes La Jetée as a &quot;photo novel&quot; rather than a film. La Jetée consists of a series of black and white photos of a post-apocoylptic Paris, a world shattered by bombs and the aftermath of human failure. They are presented in what is at times chronological and at others skipping wearily through time and space. The photos we see appear as frames from the memory of our protagonist, a prisoner, forced to revisit his past in order to find a key to secure the future of humanity. Without spoiling, the film is a existentialist journey though memory and a reflection on our own moorings to time.</p>\n<p>Without motion, La Jetée strips an additional layer of dimensionality from the presentation. We are left with words, incredible musical themes, and stills strung together through narration. Much like the unfortunate souls that have survived the final human war, we're viewers without our bearings and left with moments that appear to stand outside of time. But as the prisoner progresses further into the past to build a relationship with a woman he recognizes, we suddenly see a brief breath of frames spun together in motion for the first time.</p>\n<p>I wanted to capture this collection of fleeting moments that appear transcendental as the protagonist is transported away from the destroyed world to live in this borrowed past.</p>\n<figure>\n    <lite-vimeo videoid=\"716660835\" style=\"background-image: url('https://i.vimeocdn.com/video/716660835.webp?mw=1920&mh=1080&q=70'); aspect-ratio: 16/9;\">\n        <div class=\"ltv-playbtn\"></div>\n    </lite-vimeo>\n    <figcaption><cite>La Jetée</cite></figcaption>\n</figure>\n<p>The track I chose is a theme from the 1975 Italian film L'Uomo Dagli Occhiali A Specchio, La Notte Muore, that captures a wistful longing and melancholy. It felt right for this most hopeful of sequences in La Jetée. My main goal was to restore the sense of momentum through music. By tieing each frame with every eighth of the soundtrack, I aimed to rebuild the pace and continuity of the scene. I also decided to overlay frames on top of each other as temporal divisions, creating a feeling that each photo from the scene could have existed at any time or even simultaneously. The sequence reaches its peak with the three frames from the final scene, where we finally witness motion and the emergence of color - a sort of posthumous detachment from the familiar medium. In that moment, a blend of blue and rose fills the skin of the woman on the bed.</p>\n<h2 id=\"il-sorpasso\" tabindex=\"-1\">Il Sorpasso <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2022-08-24-film-for-music-2/#il-sorpasso\">#</a></h2>\n<p>Upon a machine-learning generated recommendation, I found Il Sorpasso - The Takeover in English, an Karaowkian Italian film out of the 60's. This post-war feeling of freedom and exploration much have been sweeping Europe as it had the US.</p>\n<p>To see more shorts from this ongoing series, you can find the showcase, hosted on Vimeo, <a href=\"https://vimeo.com/showcase/7853493\">here</a>.</p>\n",
			"date_published": "2023-02-16T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2021-05-14-meet-in-space/",
			"url": "https://jackson.gd/blog/2021-05-14-meet-in-space/",
			"title": "Toward a Telepresence of Sound: Video Conferencing in Spatial Audio",
			"content_html": "<h1 id=\"meet-in-space\" tabindex=\"-1\">Meet in Space <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#meet-in-space\">#</a></h1>\n<h2 id=\"video-conferencing-in-spatial-audio\" tabindex=\"-1\">Video Conferencing in Spatial Audio <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#video-conferencing-in-spatial-audio\">#</a></h2>\n<h3 id=\"abstract\" tabindex=\"-1\">Abstract <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#abstract\">#</a></h3>\n<p>Digital communications technologies have developed at an increasingly rapid pace, with the COVID-19 pandemic accelerating its recent adoption. This shift over the last few decades has seen a mass migration online, where utilities like video conferencing software have become essential to entire industries and institutions. Yet, there are clear limitations with this new digital work space - most of which exist from the nuance of natural communication. My master's thesis for the Music, Communications and Technology program at the University of Oslo and Norwegian University of Science and Technology proposes the integration of binaural spatialized audio within a web-based video conferencing platform for small-group, distributed conversations. The proposed system builds upon findings on the benefits of spatial audio in video conferencing platforms and is guided by the tenets of telepresence. The developed implementation is based on Jitsi Meet, a robust open-source conferencing system. It localizes participant’s voices through sound spatialization methods provided by the Web Audio API, a modern library in JavaScript for complex audio manipulation on the web.</p>\n<p>This project treads new ground in exploring how localized audio can be conceptualized within an accessible telecommunications platform, proposing a novel integration of HRTF-based binaural spatialization within a standard video conferencing layout. A novel system design and experimental questions used in a technical evaluation and user study are informed from a review of audio and video conference systems found in the literature and commercial market. The system evaluation suggests its viability from a compatibility and performance perspective. Perceptual metrics of cognitive load, social presence, and intelligibility are further investigated by a user study where four remote subjects were asked to engage in a short group discussion on a live deployment of the system. Results find support for improvements across all defined metrics as well as increased opinion scores regarding the preference of conferencing with a spatial audio system.</p>\n<h2 id=\"teleconferencing-is-here-to-stay\" tabindex=\"-1\">Teleconferencing is Here to Stay <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#teleconferencing-is-here-to-stay\">#</a></h2>\n<p>But is often a tiring affair...</p>\n<ul>\n<li>&quot;Zoom&quot; fatigue</li>\n<li>Reduced dimensionality</li>\n<li>Poor and inconsistent quality</li>\n</ul>\n<p>Teleconferencing has become apart of many of our daily rituals, either a result of the COVID epidemic or the rapid digitization of communication. However, most of us have experienced some symptom of fatigue as a result of our extended use of the system. Latency, network reliability, visual and audio fidelity can all contribute to a fatiguing experience, but software can also play a major role.</p>\n<h2 id=\"issues-in-teleconferencing\" tabindex=\"-1\">Issues in Teleconferencing <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#issues-in-teleconferencing\">#</a></h2>\n<p>My take:</p>\n<ul>\n<li>Don't reinvent the wheel - instead focus on one critical component</li>\n<li>How can the treatment of audio bring us to the goal of telepresence, and closer to realistic conversations</li>\n</ul>\n<p>There is a striking lack of realism in our video meetings, especially in how our voices are conveyed through digital exchange. This project is guided by the principles of telepresence, or the goal of conveying our sensory interactions in high fidelity, as we would in a face-to-face interaction. Every acoustic interaction we experience is spatial so aren't our digital interactions?</p>\n<h2 id=\"replicating-face-to-face-interactions\" tabindex=\"-1\">Replicating Face-to-face Interactions <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#replicating-face-to-face-interactions\">#</a></h2>\n<ul>\n<li>Spatial model, spatial audio\n<ul>\n<li>Visual-aural coherency</li>\n<li>Binaural audio</li>\n</ul>\n</li>\n<li>Benefits from the literature\n<ul>\n<li>Lateralizing audio can improve intelligibility</li>\n<li>Disentangle double-talk</li>\n<li>And more: reduce cognitive load, improve comprehension, and is generally more favorable</li>\n</ul>\n</li>\n</ul>\n<p>There are many issues that ought to be addressed in the field from a software and user experience perspective. Given that audio appears the most critical medium in task oriented communication, I chose to work on integrating spatial audio within a teleconferencing system. Spatial audio within telecommunication has a number of cool benefits and has not yet been integrated within a standard video conferencing platform.</p>\n<h2 id=\"jitsi-meet-and-web-audio\" tabindex=\"-1\">Jitsi Meet and Web Audio <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#jitsi-meet-and-web-audio\">#</a></h2>\n<p>Jitsi Meet is one of the most popular FOSS video conferencing applications that requires no sign up or installation. It works cross browser and has a great support community.</p>\n<p>To achieve spatial audio, the only current possibility for high-fidelity reproduction of sound is through binaural production of sound via headphones. Fortunately, WebAudio, a standardized method supported on all browsers, is able to faciliate full HRTF based binaural audio.</p>\n<h2 id=\"implementation\" tabindex=\"-1\">Implementation <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#implementation\">#</a></h2>\n<ul>\n<li>Capturing participants' audio streams (WebRTC)</li>\n<li>Head-related transfer functions via PannerNode</li>\n<li>Dynamic processing of participants</li>\n</ul>\n<p>With this, I integrated dynamic, toggle-able, spatial audio for each participant audio stream in a meeting. The voices of the participants appear coherent with the video streams as participants appear and disappear.</p>\n<figure>\n    <lite-vimeo videoid=\"548286337\" style=\"background-image: url('https://i.vimeocdn.com/video/810965406.webp?mw=1920&mh=1080&q=70'); aspect-ratio: 16/9;\">\n        <div class=\"ltv-playbtn\"></div>\n    </lite-vimeo>\n    <figcaption><a href=\"https://vimeo.com/548286337\">Demo of Meet in Space</a></figcaption>\n</figure>\n<h2 id=\"validating-with-a-user-study\" tabindex=\"-1\">Validating with a User Study <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#validating-with-a-user-study\">#</a></h2>\n<p>After a technical evaluation of the system suggested its ability to scale at least to 5 concurrent participants, I ran a user study consisting of students from MCT as well as other's who were experienced with conferencing on a daily basis. The experiment consisted of a brief conversation among 4 users, with and without spatial audio. The results, while preliminary, were promising and appear to support previous findings in the literature.</p>\n<p>There was support for four hypothesis of perceived metrics:</p>\n<ul>\n<li>Decreased cognitive effort</li>\n<li>Increased social presence</li>\n<li>Increased vocal intelligibility</li>\n<li>Increase in opinion score</li>\n</ul>\n<h2 id=\"and-the-future\" tabindex=\"-1\">And the future? <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2021-05-14-meet-in-space/#and-the-future\">#</a></h2>\n<p>Spatial audio is being adopted at a rapid pace, but has still yet to be introduced in small group interactions like the ones presented here. Hopefully, this thesis can provide a valid proof of concept for the benefits of spatial audio in video conferencing platforms.</p>\n",
			"date_published": "2021-05-18T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2020-10-23-film-for-music/",
			"url": "https://jackson.gd/blog/2020-10-23-film-for-music/",
			"title": "Music for a Series of Scenes",
			"content_html": "<p>Film has always been a big part of my life, as entertainment, education, and artistic inspiration. Towards the end of high school my taste in all things artistic broadened past the &quot;indie&quot; titles often found on the shelves of Urban Outfitters (forgive me) directly into the obscure. While diving into the archives of cult cinema, midnight movies and the like was entertaining on its own for the shock and awe, it was like the thrill of driving for the first time but unaware of the vistas you're passing by. Unfortunately, it took me a lot of movies to get a taste for the landscape of cinema and even now I have plenty of room to grow.</p>\n<p>As I have continued to explore the wakes of history through my 13.3in portal I've always felt some interest in the continuities between film and music. Often soundtracks highlight the emotional emphasis of a scene or tend to cast a feelings across moments or to even color the mood of the film as a whole. Less often does a film employ music that has existed prior to the film itself. There could be lots of reasons for why a director might want to avoid this: copyright/licencing, its potentially abrupt intrusion, or, what I suspect is largely the case, the meaning that a song could carry with it into the film. The appearance of a song from the past (especially if it is particularly well-known) can dramatically color the viewing experience of that scene and the movie as a whole. In cases where a song fits beautifully into a scene, you can tell the director had either imagined this song as its backdrop before getting behind the camera or was just incredibly intuitive/lucky.</p>\n<h1 id=\"a-potential-project\" tabindex=\"-1\">A potential project <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-23-film-for-music/#a-potential-project\">#</a></h1>\n<p>This dilemma has led me to imagine the possibilities of re-contextualizing films under new soundtracks, perhaps ones made even after the film. This pieces would have the benefit of extending upon not just the legacy of the song, as a director would if they included it in their film, but also the film itself. This imagined production would appropriate the history of the film as well the song that was chosen to fit the scene. I imagined that these shorts would be akin to painting with a new pallette - brief glimpses of a familiar world (sonic and visually) that was re-imagined through new positionings in sound, image, and history. These short clips would also serve to highlight potentially uncommon works in ways that might elevate them, inject a rush of novelty, or critically expose angles of the works that could not be focused on in a standard viewing.</p>\n<p>I hope to continue making these shorts both for satisfying ideas I've had in the log and for getting some experience with video editing. In this first blog, I'll talk about two of the shorts I've done and the stories that accompany my choices.</p>\n<h2 id=\"funeral-parade-of-roses\" tabindex=\"-1\"><em>Funeral Parade of Roses</em> <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-23-film-for-music/#funeral-parade-of-roses\">#</a></h2>\n<figure>\n    <lite-vimeo videoid=\"457926908\" style=\"background-image: url('https://i.vimeocdn.com/video/810965406.webp?mw=1370&mh=1000&q=70'); aspect-ratio: 1.37/1;\">\n        <div class=\"ltv-playbtn\"></div>\n    </lite-vimeo>\n    <figcaption><cite>Funeral Parade of Roses</cite></figcaption>\n</figure>\n<p>One of my favorite films that I've seen over the last few years, and beyond that, is the first feature length film by art-house director Toshio Matsumoto. The film was firmly situated in the <a href=\"https://en.wikipedia.org/wiki/Japanese_New_Wave\">Japanese New Wave</a> led by the Art Theatre Guild production company. To say that <em>Funeral Parade of Roses</em> is unlike most films is an understatement. Without giving too much away, Roses follows the life of Peter, a young transvestite (or more likely transgender, in a modern context) through the underground gay scene of 1960's Tokyo.</p>\n<p>Even more unusual is that many scenes from the film interview and discuss the lives of the actors, their thoughts and opinions of the film they're working in, and even include shots of the film set. But these scenes are not the only that break from the standard plot, there are countless sequences that flirt with mischief, eroticism, and terror in a bold visual style all their own. There's also a dash of Greek mythology and drug use - 1969 seemed to be a wild year everywhere.</p>\n<p>For Roses, I ended up landing on a an artist I had been introduced to by the director Apichatpong Weerasethakul in the erie, final scene of his last film <em>Cemetary of Splendour</em>. DJ Soulscape is a Korean producer whose 2003 album <em>Lovers:</em> is a collection of samples ranging from downbeat electronic to hip-hop - navigating between the legacies of Nujabes and Air. The first track off Lovers:, &quot;People&quot; begins with a piano bang, reminiscent of &quot;A Day in the Life&quot;, and builds over new samples in each measure. It's a wonderfully cinematic track and really grooves when the bass line kicks in.</p>\n<h2 id=\"suspiria\" tabindex=\"-1\"><em>Suspiria</em> <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-23-film-for-music/#suspiria\">#</a></h2>\n<figure>\n    <lite-vimeo videoid=\"471716567\" style=\"background-image: url('https://i.vimeocdn.com/video/810965406.webp?mw=2350&mh=1000&q=70'); aspect-ratio: 2.35/1;\">\n        <div class=\"ltv-playbtn\"></div>\n    </lite-vimeo>\n    <figcaption><cite>Suspiria</cite></figcaption>\n</figure>\n<p><em>Suspiria</em> was a film that I had wanted to see for a while. An Italian film with an American lead actress and casting from both Italy and Germany, it was quite an ambitious project and one where many of the actors simply spoke in their native language to be overdubbed. But besides the logistics, <em>Suspiria</em> is known as a staple of the euro-horror genre.</p>\n<p>Compared to <em>Roses</em>, the music in this case actually informed the choice of the film. The song chosen comes from Claudine Longet, a French-American popular for covering American and Brazilian standards in the 60's and 70's as well as for being married to Andy Williams for a decade in that period. However, her career was cut short by a murder trial of her new boyfriend in Aspen, 1976. This was a widely publicized trial, mired by slipups from the persecution and the police and ended with a light sentence of a month in jail, a small fine, and with breaks on the weekend to see her kids. Even more incredible was that Longet made off with her defense attorney post-trail. They still live in Aspen.</p>\n<p>Given the hefty settlements made to the victims family later, the validity of the trail is heavily disputed. The legacy of Claudine Longet that creeps under her lighthearted vocal performances made a perfect mix for a horror film like <em>Suspiria</em>. &quot;Here, There and Everywhere&quot; is a track written by The Beatles' Paul McCartney - one of his favorites and covered widely. Longet's take (or likely Tommy LiPuma's, as producer) is quite strange, with syncopated drums and pizzicato violins, quiet guitar and a single, haunting lead vocal.</p>\n<h2 id=\"for-more\" tabindex=\"-1\">For more <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-23-film-for-music/#for-more\">#</a></h2>\n<p>To see more shorts from this ongoing series, you can find the showcase, hosted on Vimeo, <a href=\"https://vimeo.com/showcase/7853493\">here</a>.</p>\n",
			"date_published": "2020-10-23T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2020-10-16-musings-bela/",
			"url": "https://jackson.gd/blog/2020-10-16-musings-bela/",
			"title": "Musings with Bela",
			"content_html": "<p>In an effort to explore the wild world of interactive music systems, I decided to work with a <a href=\"https://choosemuse.com/\">portable EEG reader</a> and a <a href=\"https://bela.io/\">Bela</a> coupled with an accelerometer and potentiometers. Little did I know how much of a challenge it would be to join both software and hardware within an interactive package.</p>\n<h2 id=\"inspiration\" tabindex=\"-1\">Inspiration <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#inspiration\">#</a></h2>\n<p>Coming from a undergraduate degree in cognitive science, I've always wanted to work with (read hack) an <a href=\"https://en.wikipedia.org/wiki/Electroencephalography\">electroencephalogram</a> (EEG) for some kind of artistic performance or instrument. While I have worked with a more traditional systems that require head-caps and conductive gel, I have never had the opportunity to test out some of the many different portable systems that have come out in the last decade. After reaching out to <a href=\"https://arj.no\">Alexander Jensenius</a>, I was able to borrow a system from a researcher at RITMO - which I'll cover later in the hardware section.</p>\n<h3 id=\"artistic-examples\" tabindex=\"-1\">Artistic examples <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#artistic-examples\">#</a></h3>\n<p>In addition to my personal interest in finding an artistic meeting point between cog-sci and MCT, I was also inspired by a number of performances and musical systems that employed EEG technology.</p>\n<p>The first of these was <a href=\"https://en.wikipedia.org/wiki/Alvin_Lucier\">Alvin Lucier's</a> &quot;Music for Solo Performer&quot; (1965) which was a landmark piece not only for its use of an EEG but sonification generally. Lucier had mapped the voltage potential from his electrodes into low-frequency tones that were able to excite percussive instruments in front of him.</p>\n<figure>\n    <lite-youtube videoid=\"bIPU2ynqy2Y\" style=\"background-image: url('https://i.ytimg.com/vi/bIPU2ynqy2Y/hqdefault.jpg');\" title=\"Alvin Lucier's Music for Solo Performer (1965)\">\n        <a href=\"https://youtube.com/watch?v=bIPU2ynqy2Y\" class=\"lty-playbtn\" title=\"Alvin Lucier's Music for Solo Performer (1965)\">\n            <span class=\"lyt-visually-hidden\">Alvin Lucier's Music for Solo Performer (1965)</span>\n        </a>\n    </lite-youtube>\n</figure>\n<p>The second performance that offered insights into using EEG's in a sonic environment was Ouzounian et al.'s Music for Sleeping &amp; Waking Minds (2011-2012). In their piece, they asked a group of participants to wear EEG sensors as they spent a night in a collective slumber. Over the course of the night, their brain waves (in passing through the various oscillatory states of sleep) were represented in sound and light.</p>\n<figure>\n    <div class=\"iframe-wrapper\">\n        <iframe src=\"https://player.vimeo.com/video/30261043?byline=0&portrait=0\" width=\"560\" height=\"315\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe>\n    </div>\n    <figcaption>Participants sleeping for Music for Sleeping & Waking Minds (2011-2012)</figcaption>\n</figure>\n<p>In addition to these, there were a number of other interesting takes on EEG sonification such as <a href=\"http://graceleslie.com/MoodMixer\">MoodMixer</a> (Leslie and Mullen, 2011), a collaborative installation where two participants navigate a shared musical space via EEG as represented by a 2D visual space. Another implementation comes from the <a href=\"https://www.researchgate.net/publication/209435991_Disembodied_and_Collaborative_Musical_Interaction_in_the_Multimodal_Brain_Orchestra\">Multimodal Brain Orchestra</a> (Le Grouz et al., 2010), a collection of musicians whose sheet music was generated on the spot as a product from a reading of their collective cognitive response. And recently, <a href=\"https://www.youtube.com/watch?v=n0T2uB-GLc8\">Chris Chafe</a> worked on sonification of seizure data recorded from EEGs, providing an illumination of hidden neural activity.</p>\n<p>However, in all of these examples, it's not obvious what the sensor data is actually being mapped to - a confusing experience from both the audience as well as someone trying to find inspiration for an IMS of their own. For a much more clear framework on how one would go about building and evaluating an IMS, I turned to the literature.</p>\n<h3 id=\"academic-support\" tabindex=\"-1\">Academic support <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#academic-support\">#</a></h3>\n<p>Two articles held my interest during the time I spent conceptualizing and designing my system, the first from Birnbaum et al. In their article Towards a Dimension Space for Musical Devices, the authors lay out a visual representation for describing aspects of IMS (Birnbaum et al., 2005). They identify 7-axes that might characterize new interactive music systems and, in parallel, provide some representative space of which to locate an author's proposal for their own IMS.</p>\n<ul>\n<li>Required Expertise</li>\n<li>Musical Control</li>\n<li>Feedback Modalities</li>\n<li>Degrees of Freedom</li>\n<li>Inter-actors</li>\n<li>Distribution in Space</li>\n<li>Role of Sound</li>\n</ul>\n<p>These principles were helpful as a means of comparing my proposed instrument against others but also for a class of features to focus on as I developed it. For Musings with Bela, this is how we might visualize its capacity as an instrument:</p>\n<figure><img src=\"https://jackson.gd/img/musings-dim_space.png\" width=\"560px\" alt=\"Musings with Bela's dimension space\"><figcaption>Musings with Bela's dimension space</figcaption></figure>\n<p>The second paper, A Framework for the Evaluation of Digital Musical Instruments by O'Modhrain takes ups a similar issue with the absence of well defined lenses through which we can consider, criticize and explain a musical instrument (O'Modhrain, 2011). O'Modhrain suggest that taking the perspective of not only a musician or designer when building an IMS, but also that of an audience member or even a manufacturer. These novel perspectives force an author to consider their instrument from angles that are not typically confronted until after the instrument has been built. In both articles, it is clear that building a musical interface is a project whose treatment must be considered with others in mind.</p>\n<h2 id=\"hardware\" tabindex=\"-1\">Hardware <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#hardware\">#</a></h2>\n<p>The device actually made use of the breadboard it was wired to as a frame to hold and rotate the device. The Bela was placed in between the accelerometer and two knobs, allowing for it to easily sit in one's hands. The idea was to keep the design uncomplicated as the EEG might require the performer to have their eyes closed.</p>\n<div class=\"flex\">\n    <figure class=\"split-view\">\n        <img src=\"https://jackson.gd/img/musings-bela.jpg\">\n        <figcaption>Bela and friends</figcaption>\n    </figure>\n    <figure class=\"split-view\">\n        <img src=\"https://jackson.gd/img/musings-muse.jpg\">\n        <figcaption>The Muse</figcaption>\n    </figure>\n</div>\n<p>A <a href=\"https://choosemuse.com/\">Muse</a> (2016) portable eeg headband, graciously borrowed from RITMO, was another major hardware device incorporated within my IMS. I had read through my preliminary research that this device might be easily hackable. Imagine my frustration after finding out all developer resources for the device were discontinued in the last year. Unbroken, I pushed forward and ended up modifying a completely unknown Python package to finally interface with the device.</p>\n<p>Nevertheless, the device's specs were quite impressive with 4 electrodes recording at 256Hz and an all-day battery life (no joke). Unfortunately, the fact that the device streamed through Bluetooth meant that my laptop would necessarily be involved (I wouldn't <em>dare</em> attempt it on the Bela (Okay, maybe if I had another week!)).</p>\n<h2 id=\"software\" tabindex=\"-1\">Software <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#software\">#</a></h2>\n<h2 id=\"interpolating-between-tables-with-an-accelerometer\" tabindex=\"-1\">Interpolating between tables with an accelerometer <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#interpolating-between-tables-with-an-accelerometer\">#</a></h2>\n<figure><img src=\"https://jackson.gd/img/musings-tables.webp\" width=\"420px\" alt=\"Sliding between different tables\"><figcaption>Sliding between different tables</figcaption></figure>\n<p>At the core of my system was a method for interpolating between short audio grains. Audio files were read into an array of 1024 samples and these arrays were then interpolated using the external <a href=\"https://jackson.gd/blog/2020-10-16-musings-bela/\">iemmatrix</a>. More typical methods of reading through arrays would be to step through each index and read the sample, apply whatever operation you wanted and then store it. In my case, however, I wanted to tie the accelerometer to degree each sound file is interpolated into one another (via arrays) which makes it challenge to read through these arrays sequentially when the sample rate of change needs to be very fast. iemmatrix instead, allows for operations to take place on the array as a whole (like <a href=\"https://www.geeksforgeeks.org/vectorization-in-python/\">numpy vectorization</a>) meaning this is a much more efficient method of morphing between these arrays.</p>\n<figure><img src=\"https://jackson.gd/img/musings-intrp.png\" width=\"560px\" alt=\"A shot of the main matrix operation sub-patch\"><figcaption>A shot of the main matrix operation sub-patch</figcaption></figure>\n<p>What's cool about this is that working with sound files as tables allows you to do some non-linear transformations like interpolating between a sound and it's reverse sequence.</p>\n<figure><img src=\"https://jackson.gd/img/musings-reverse.webp\" width=\"420px\" alt=\"Morphing between a sound bite and its flipped image\"><figcaption>Morphing between a sound bite and its flipped image</figcaption></figure>\n<p>Upon reflection, another alternative would be to get two readings, do the matrix operations, and slide between them with a [line] object. As I was looking into this, this is an external (list-abs) that allows for linear interpolation between lists. However, this might be a slightly more costly object to use - perhaps a combination of both techniques would have worked best.</p>\n<p>The two physical knobs control an oscillator to read the resultant table (a morphed sound grain) into the DAC. These knobs are also read at audio-rate but the operations they control are far less complicated. In Musings with Bela, these knobs serve as tuners for the synth that can explored (sonically) by rotating the device.</p>\n<h2 id=\"reading-arrays-and-remaining-calm\" tabindex=\"-1\">Reading arrays and remaining calm <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#reading-arrays-and-remaining-calm\">#</a></h2>\n<p>Finally, the last piece to my glorious IMS puzzle, the Muse! As I mentioned, this device was tricky, thorny, and a general struggle to work with, especially considering I had to set the Bela up as a WiFi hot-spot to pass the samples from the Muse, to my PC and then off to the Bela.</p>\n<p>Another major piece of working with the Muse was actually testing to see the behavior of the electrode readings: if they were consistent, their fluctuations, and whether or not I would be able to reliably reduce the noise and relative intensity. I made a sub-patch to test for this reason, allowing me to record and playback samples from the Muse even without its connection.</p>\n<figure><img src=\"https://jackson.gd/img/musings-eeg.webp\" width=\"100%\" alt=\"Data from the Muse\"><figcaption>Data from the Muse</figcaption></figure>\n\n<p>Musings with Bela takes this EEG stream and modulate the amplitude of the read table so that, in theory, a wandering, active mind would lead to a disrupted synth. The configuration was technical to say the least and, in retrospect, something I wish I had tackled earlier in the building process so I could use the EEG signals in a more complex mapping.</p>\n<h2 id=\"reflections\" tabindex=\"-1\">Reflections <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#reflections\">#</a></h2>\n<p>Building an IMS is a whirlwind of an experience and one that is especially difficult to achieve in two weeks. Working with hardware and software turned out to double the time I expected any individual task would take. However, I feel like I had successfully built an interesting system that touched on my history with cognitive science and applied within an acousmatic environment.</p>\n<p>Here is my short, final performance for MCT4045</p>\n<figure>\n\t\t\t\t<lite-youtube videoid=\"gEq9EnWrApc?start=901\">\n\t\t\t\t\t<button type=\"button\" class=\"lty-playbtn\">\n\t\t\t\t\t\t<span class=\"lyt-visually-hidden\">Musings with Bela Performance</span>\n\t\t\t\t\t</button>\n\t\t\t\t</lite-youtube>\n\t\t\t\t<figcaption>Musings with Bela Performance</figcaption>\n\t\t\t</figure>\n<p>And my final presentation can be found below as well</p>\n<figure>\n<iframe src=\"https://slides.com/jacksongoode/musings-bela/embed\" width=\"800\" height=\"450\" scrolling=\"no\" frameborder=\"0\" webkitallowfullscreen=\"\" mozallowfullscreen=\"\" allowfullscreen=\"\"></iframe>\n</figure>\n<h2 id=\"references\" tabindex=\"-1\">References <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-10-16-musings-bela/#references\">#</a></h2>\n<p>Birnbaum, David, et al. <em>Towards a Dimension Space for Musical Devices</em>. 2005.</p>\n<p>Fan, Yuan-Yi, and F. Myles Sciotto. 'BioSync: An Informed Participatory Interface for Audience Dynamics and Audiovisual Content Co-Creation Using Mobile PPG and EEG.' <em>NIME</em>, 2013, pp. 248--251.</p>\n<p>Hamano, Takayuki, et al. 'Generating an Integrated Musical Expression with a Brain-Computer Interface.' <em>NIME</em>, 2013, pp. 49--54.</p>\n<p>Le Groux, Sylvain, et al. 'Disembodied and Collaborative Musical Interaction in the Multimodal Brain Orchestra.' <em>NIME</em>, 2010, pp. 309--314.</p>\n<p>Leslie, Grace, and Tim R. Mullen. 'MoodMixer: EEG-Based Collaborative Sonification.' <em>NIME</em>, Citeseer, 2011, pp. 296--299.</p>\n<p>O'Modhrain, Sile. 'A Framework for the Evaluation of Digital Musical Instruments'. <em>Computer Music Journal</em>, vol. 35, Mar. 2011, pp. 28--42. <em>ResearchGate</em>, doi:<a href=\"https://doi.org/10.1162/COMJ_a_00038\">10.1162/COMJ_a_00038</a>.</p>\n<p>Ouzounian, Gascia, et al. 'To Be inside Someone Else's Dream: On Music for Sleeping &amp; Waking Minds'. <em>New Interfaces for Musical Expression (NIME 2012)</em>, 2012, pp. 1--6.</p>\n<p>Parvizi, Josef, et al. 'Detecting Silent Seizures by Their Sound'. <em>Epilepsia</em>, vol. 59, no. 4, 2018, pp. 877--84. <em>Wiley Online Library</em>, doi:<a href=\"https://doi.org/10.1111/epi.14043\">10.1111/epi.14043</a>.</p>\n<p>Straebel, Volker, and Wilm Thoben. 'Alvin Lucier's Music for Solo Performer: Experimental Music beyond Sonification'. <em>Organised Sound</em>, vol. 19, no. 1, Cambridge University Press, Apr. 2014, pp. 17--29. <em>Cambridge University Press</em>, doi:<a href=\"https://doi.org/10.1017/S135577181300037X\">10.1017/S135577181300037X</a>.</p>\n<p>Wu, Dan, et al. 'Scale-Free Brain Quartet: Artistic Filtering of Multi-Channel Brainwave Music'. <em>PloS One</em>, vol. 8, no. 5, 2013, p. e64046. <em>PubMed</em>, doi:<a href=\"https://doi.org/10.1371/journal.pone.0064046\">10.1371/journal.pone.0064046</a>.</p>\n",
			"date_published": "2020-10-16T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/",
			"url": "https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/",
			"title": "Classifying Urban Sounds in a Multi-label Database",
			"content_html": "<h1 id=\"environmental-sound-classification-over-concurrent-samples\" tabindex=\"-1\">Environmental Sound Classification over Concurrent Samples <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/#environmental-sound-classification-over-concurrent-samples\">#</a></h1>\n<p>Given the short two week span to develop a machine learning model, I decided instead of beginning anew, to repurpose some pre-existing methods in approaching environmental sound classification. Environmental sound classification (ESC) is a field that benefits well from machine learning techniques, as the data examined will always be unique and noisy. Two well known databases, UrbanSound8K (US8K) [5] and ESC-50 [9] provide recordings from <a href=\"https://freesound.org\">Freesound.org</a>, trimmed, labeled and grouped into categories for analysis. This system attempts to utilise a <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">convolutional neural network</a> (CNN) on an augmented UrbanSound8K dataset for multi-label classification.</p>\n<p><a href=\"https://urbansounddataset.weebly.com/urbansound8k.html\">UrbanSound8K</a> contains over 8000 sound files separated by categories of sounds typically found in an urban setting. Instead of exclusive categorical labels in its original state, the dataset has been recreated with the purpose of exploring how a successful architecture might perform on multi-label samples rather than simply uni-label, multi-class sounds. Thus, this project investigates how techniques in classifying an environmental noise database might generalize to a multi-label scenario with a new database composed of overlaid sound pairs.</p>\n<figure>\n    <img alt=\"UrbanSound8K classes\" src=\"https://jackson.gd/img/us8k_spec.jpg\">\n    <figcaption>Spectrograms of three UrbanSound8K classes</figcaption>\n</figure>\n<p>The initial code that I forked was sourced from <a href=\"https://github.com/aqibsaeed/Urban-Sound-Classification\">Aapid Saeed’s implementation</a> which was in turn inspired by Karol Piczak’s 2015 paper that provides a scientific example of ENC using a CNN [4]. The experiment tested here provides some insight into the obstacles that appear when shifting this problem space both in terms of performance but also how features and parameters must be resituated.</p>\n<h2 id=\"overview-of-dataset-and-fabrication\" tabindex=\"-1\">Overview of dataset and fabrication <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/#overview-of-dataset-and-fabrication\">#</a></h2>\n<p>The original dataset, UrbanSound8K, contains 8732 .wav sounds sourced from Freesound.org across 10 classes of urban sounds:</p>\n<ol>\n<li>air conditioner</li>\n<li>car horn</li>\n<li>children playing</li>\n<li>dog bark</li>\n<li>drilling</li>\n<li>engine idling</li>\n<li>gun shot</li>\n<li>jackhammer</li>\n<li>siren</li>\n<li>street music</li>\n</ol>\n<p>All are separated into 10 folds (each containing an equal distribution selection of the classes). These sounds are all less than 4s but vary in length, recording device, sample rate and perceptual loudness. Many of the sounds were generated as slices from longer sounds - meaning that many sounds share the same sound file source.</p>\n<p>For creating a new dataset, I composed of multiple tracks of audio from UrbanSound8K. Using the library pydub, I created a separate script to use a randomized list of each fold’s files and overplayed each file with a randomized gain reduction between -6 and 0. This enabled a more authentic mixture of sound in a live context and would also contribute to the robustness of the model and its consequent difficulty during training (10 classes to 45 conceptual classes (or 10 choose 2)).</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n<span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>multi_num<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># number of samples to mix</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Processing: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>samples<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n\n    labels<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> samples<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">'-'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token comment\"># get label</span>\n    sounds<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> effects<span class=\"token punctuation\">.</span>normalize<span class=\"token punctuation\">(</span>sounds<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># normalize</span>\n    sounds<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> sounds<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> rand_gain<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token comment\"># add random gain reduction</span>\n    p_ratio<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token builtin\">pow</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> rand_gain<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token operator\">/</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># convert to power</span>\n    names<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f'-</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>labels<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">(</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>p_ratio<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">)'</span></span> <span class=\"token comment\"># label file with params</span>\n\ncombined <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\ncombined<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>sounds<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>overlay<span class=\"token punctuation\">(</span>sounds<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> times<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># overlay sound (repeat if base sound is longer)</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre>\n<p>Two tracks of audio were chosen after testing the capabilities with three concurrent sounds, which appeared even too difficult to discern by ear - however, the script was written with the possibility of any number of overlays. This brings up another point - some of these classes of sound like 'air conditioner' and 'engine idling' closely resembled white noise, something that every recording ultimately contains due to the nature of recording sound. The final “multi” database is a little less than half the size of US8K as some of the samples of US8K were unreadable and were skipped.</p>\n<h2 id=\"implementation-of-model\" tabindex=\"-1\">Implementation of model <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/#implementation-of-model\">#</a></h2>\n<p>In addition to the creation of a script to fabricate and label a multi-labeled dataset, I augmented the scripts used to evaluate a CNN on the original UrbanSound8K database to suit this new multi-label scenario. The actual feature processing was identical to the one-sound-per-sample database. The audio data’s features (mel-spectrogram) were extracted and filtered with <a href=\"https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html?highlight=mel#librosa.feature.melspectrogram\">librosa</a>.</p>\n<figure>\n    <img width=\"560px\" alt=\"One example of the mel-spectrogram feature\" src=\"https://jackson.gd/img/spec_1.png\">\n    <figcaption>One example of the mel-spectrogram feature</figcaption>\n</figure>\n<figure>\n    <img width=\"560px\" alt=\"Another example\" src=\"https://jackson.gd/img/spec_2.png\">\n    <figcaption>Another example</figcaption>\n</figure>\n<p>Most of my efforts here went into changing how the labels were being processed. The network used to train and test the data was fashioned with the Keras wrapper for TensorFlow and other ML backends. Keras was chosen as it offers the ability to construcasdast neural networks at a lower level than the sklearn package whilst being fairly novice-friendly.</p>\n<h2 id=\"model-tuning-for-new-dataset\" tabindex=\"-1\">Model tuning for new dataset <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/#model-tuning-for-new-dataset\">#</a></h2>\n<p>Parameters that were previously provided by Saeed needed to be adjusted considering the new multi-label problem space. The first major change needed to happen at the label encoding level as labels were no longer a number from 0-9, an array of n values from 0-9. To resolve this, the labels were transformed into one-hot binary encoding. This allowed multiple simultaneous classes to be represented in an array of the same length.</p>\n<p>Another major change concerned the end of the network, where predictions and evaluations are made on the training data. The final dense layer was set to the softmax activation function which was <a href=\"https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/\">inappropriate for a non-exclusive multi-label scenario</a>. The softmax function outputs probabilities for classes that sum to one, making it impossible for multiple classes to reach a binary activation. In this case, sigmoid offers probability distributions that are unconstrained, enabling multiple classes to reach binary classification.</p>\n<figure>\n    <img width=\"560px\" alt=\"Applications of softmax vs. sigmoid\" src=\"https://miro.medium.com/max/1268/1*-fADiyqSH9AHPPO7VB4XaQ.png\">\n    <figcaption>Applications of softmax vs. sigmoid - <a href=\"https://medium.com/aidevnepal/for-sigmoid-funcion-f7a5da78fec2\">Credit to Ashis Parajuli</a></figcaption>\n</figure>\n<p>In parallel, the loss function needed to be changed from categorical cross entropy to binary cross-entropy for this binary data.</p>\n<h2 id=\"evaluation-of-performance\" tabindex=\"-1\">Evaluation of performance <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/#evaluation-of-performance\">#</a></h2>\n<p>During training, the accuracy score increased to around 44% after 15 epochs while the loss continuously decreased. However, the last 10 epochs showed only a 4% increase in accuracy, suggesting the model was approaching convergence of the weight values and perhaps over-fitting (I had also attempted with longer training sessions with similar outcomes).</p>\n<figure>\n    <img alt=\"Categorical accuracy over epochs\" src=\"https://jackson.gd/img/cat_acc.png\" width=\"560px\">\n    <figcaption>Categorical accuracy over epochs</figcaption>\n</figure>\n<figure>\n    <img alt=\"Model loss over epochs\" src=\"https://jackson.gd/img/model_loss.png\" width=\"560px\">\n    <figcaption>Model loss over epochs</figcaption>\n</figure>\n<p>In testing the data, again the migration from a single categorical label to a one hot binary array means that the accuracy metric (a mean average of predictions on the test set) does not give us the whole picture. Moving to <a href=\"https://en.wikipedia.org/wiki/Hamming_distance\">hamming loss</a>, an estimate that shows use what percent of our answer's elements were correct, is a much better metric when predicting on a multi-label sample.</p>\n<p>The average accuracy was quite poor, as expected, sitting at 18%, however, the hamming loss (less is better) was 15%, meaning that 85% of the model's label predictions were correct. These metrics are in sharp contrast to the performance of the untampered UrbanSound8K dataset, which was observed to have an accuracy of around 75-85% for most state of the art models [1, 2, 4, 8].</p>\n<figure>\n    <img alt=\"Confusion matrix across the ten classes\" src=\"https://jackson.gd/img/confusion_matrix.png\" width=\"480px\">\n    <figcaption>Confusion matrix across the ten classes</figcaption>\n</figure>\n<p>The confusion matrix and classification report, of one training instance, also provide interesting insights into how the test data was predicted. One major point to note is the time scale of some of these classes. It appears that those sounds that appear briefly with high impulses like the car honk, dog bark, and gunshot (1, 3, 6) are some of the most precisely predicted classes (low-false positives).</p>\n<p>And as one might expect, the classes most difficult to label correctly turn out to be the noisiest and likely most organic and spectrally dynamic sounds within US8K: children playing, air conditioner and street music corresponding to 2, 0, 9. Of course, these metrics would need to be compared to the performance of the model on the raw UB8K database for one to make clear conclusions about how the shift into a multi-labeled dataset</p>\n<h2 id=\"reflections\" tabindex=\"-1\">Reflections <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/#reflections\">#</a></h2>\n<p>Not all of the details of the network employed have been fully elaborated and it may be that some transformations of the input data have been overlooked. Given the provided window of time and simultaneous introduction to techniques in machine learning, this investigation fulfils, at least, a tentative exploration into the field of ML based ENC.</p>\n<p>One obvious challenge in mixing sounds as was performed in this experiment is the perceptual presence of the sound within the sample. This is actually accounted for by a subjective estimate in the taxonomy of the UrbanSound8K database where they determine if the category was a foreground or background sound. For categories like “air_conditioner” and “engine_idling”, poor classification performance would be expected when mixing sounds of these classes due to their lack of a sonic shape - they mostly consist of white noise. One might predict then that these categories were over-predicted on average across all sounds. Indeed, that does appear to be the case and this can be seen through the confusion matrix.</p>\n<p>Another issue tied to the source database was its sheer size. The total time required to load around 9000 .wav files makes it prohibitively expensive when tuning parameters, or in this case, adapting the processing of files for a new problem space. K-fold validation would have been helpful in estimating the average accuracy of the model and providing greater confidence in our reflections of the model but there was not enough time to do so.</p>\n<p>Code and instructions for setting up this project can be found <a href=\"https://github.com/jacksongoode/enc-multi-label\">here</a>.</p>\n<h2 id=\"references\" tabindex=\"-1\">References <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-09-20-classifying-urban-sounds/#references\">#</a></h2>\n<ol>\n<li>\n<p>Abdoli, S., Cardinal, P., &amp; Koerich, A. L. (2019). End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network. ArXiv:1904.08990 [Cs, Stat]. <a href=\"http://arxiv.org/abs/1904.08990\">http://arxiv.org/abs/1904.08990</a></p>\n</li>\n<li>\n<p>Mushtaq, Z., &amp; Su, S.-F. (2020). Environmental sound classification using a regularized deep convolutional neural network with data augmentation. Applied Acoustics, 167, 107389. <a href=\"https://doi.org/10.1016/j.apacoust.2020.107389\">https://doi.org/10.1016/j.apacoust.2020.107389</a></p>\n</li>\n<li>\n<p>Piczak, K. J. (2020). Karolpiczak/ESC-50 [Python]. <a href=\"https://github.com/karolpiczak/ESC-50\">https://github.com/karolpiczak/ESC-50</a> (Original work published 2015)</p>\n</li>\n<li>\n<p>Piczak, K. J. (2015a). Environmental sound classification with convolutional neural networks. 2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. <a href=\"https://doi.org/10.1109/MLSP.2015.7324337\">https://doi.org/10.1109/MLSP.2015.7324337</a></p>\n</li>\n<li>\n<p>Piczak, K. J. (2015b). ESC: Dataset for Environmental Sound Classification. Proceedings of the 23rd ACM International Conference on Multimedia, 1015–1018. <a href=\"https://doi.org/10.1145/2733373.2806390\">https://doi.org/10.1145/2733373.2806390</a></p>\n</li>\n<li>\n<p>Saeed, A. (n.d.). Urban Sound Classification, Part 2. Retrieved 18 September 2020, from <a href=\"http://aqibsaeed.github.io/2016-09-24-urban-sound-classification-part-2/\">http://aqibsaeed.github.io/2016-09-24-urban-sound-classification-part-2/</a></p>\n</li>\n<li>\n<p>Aaqib. (2020). Aqibsaeed/Urban-Sound-Classification [Jupyter Notebook]. <a href=\"https://github.com/aqibsaeed/Urban-Sound-Classification\">https://github.com/aqibsaeed/Urban-Sound-Classification</a> (Original work published 2016)</p>\n</li>\n<li>\n<p>Su, Y., Zhang, K., Wang, J., &amp; Madani, K. (2019). Environment Sound Classification Using a Two-Stream CNN Based on Decision-Level Fusion. Sensors, 19(7), 1733. <a href=\"https://doi.org/10.3390/s19071733\">https://doi.org/10.3390/s19071733</a></p>\n</li>\n<li>\n<p>UrbanSound8K. (n.d.). Urban Sound Datasets. Retrieved 23 August 2020, from <a href=\"https://urbansounddataset.weebly.com/urbansound8k.html\">https://urbansounddataset.weebly.com/urbansound8k.html</a></p>\n</li>\n</ol>\n",
			"date_published": "2020-09-20T20:30:20Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2020-05-20-breathing-through-max/",
			"url": "https://jackson.gd/blog/2020-05-20-breathing-through-max/",
			"title": "Breathing through Max",
			"content_html": "<h2 id=\"rationale\" tabindex=\"-1\">Rationale <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-05-20-breathing-through-max/#rationale\">#</a></h2>\n<p>Analyzing motion from smartphone sensors or laptop cameras is a both a challenging scenario and exciting opportunity. This project transforms the live acceleration data recorded from an iPhone resting on the torso of a person laying down into a dynamic sonic atmosphere that may enhance the monitoring one’s breath through auditory cues. One potential use of this system would be as a meditation or relaxation assistant. The sound environment aims to be soothing and ought to coax one into a deeper and more rhythmic breathing cycle. There is ample empirical data to suggest that paced respiration can reduce anxiety and other measures of perceived stress. Following common themes of this literature, <a href=\"https://en.wikipedia.org/wiki/Biofeedback\">biofeedback</a> is a topic of importance in recognizing the effects of this system on a user.</p>\n<p>The inhales and exhales are identified as peaks and troughs within the accelerometer data and as a result, an average breathing rate can be obtained that serves as a marker of how consistently rhythmic the individual’s breathing rate is. This indicator is used to grow and shrink the sonic environment as one’s breathing and body begin to settle. On the surface, the information gained from the chest’s rise and fall appear to be an easy to manipulate stream. However, most of the time spent on the project involved developing a reliable and flexible model for processing a very noisy stream of data due to the micro-movements that were being recorded. Various parameters of the system were then modulated by other much more noisy sensor data as well.</p>\n<h2 id=\"methodology\" tabindex=\"-1\">Methodology <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-05-20-breathing-through-max/#methodology\">#</a></h2>\n<p>The first task before embarking on this project was finding a suitable app that would be a reliable recorder and streaming platform for micro-motion data. Sadly, there are not many apps that provide a data logging functionality on iOS and even fewer that provide a way to record data from the sensors and only one that appears to meet the requirements I had for such an app,</p>\n<ol>\n<li>Log data at a high frequency (&gt; 50hz)</li>\n<li>Record multiple sensors at once</li>\n<li>Export this data in CSV format and</li>\n<li>Send this data over a networked connection</li>\n</ol>\n<p>Most of these apps transmit their sensor data over a UDP connection using the OSC protocol. This type of communication is well supported by many DAWs and platforms like Max and Pure Data. However, the app that was used, SensorLog unfortunately, did not send data with the OSC protocol. Instead, it sends the data over UDP as it would when writing a CSV file that was not compatible with Max’s [udpreceive] object.</p>\n<p>To work around this, I wrote a python script that acted as a node to receive, format the stream into proper OSC messages, and finally, send the data locally to a specified port within Max. The script operated by receiving a row of data, splitting the list of data points into an array, and then sending each index of the array as a pair with a specified OSC message. I used the messages “/x”, “/y”, and “/z” as shorthand for the x, y, z, acceleration data. Once this was passed into Max, the data was able to be handled the same way as any other OSC message.</p>\n<figure><img src=\"https://jackson.gd/img/breathing-code.png\" width=\"100%\" alt=\"Sample of the code used to transport the messages into Max\"><figcaption>Sample of the code used to transport the messages into Max</figcaption></figure>\n<p>After some initial testing of various sensors and positions during recording, I found that the acceleration value for the y-axis was the most reliable source of information for tracking respiratory motion. Because of the placement of the phone, none of the other sensors render any useful information. The only other sensor that appeared to follow the same information was the gyroscope, but the sensor upon inspection looked like a transformation of the acceleration data. In the first trial recording set of breathing, I was able to find that this sensor’s y-axis extracted recognizable oscillations of the rise and fall of my stomach. However, because the movements were quite small, the noise of the sensor clouded what was, in reality, a smooth envelope of motion.</p>\n<p>Reading the CSV data into Max was achieved by loading each line of the CSV as a text file from which I could use a [metro] object to iterate through the [coll] dictionary that stored the sensor data (<a href=\"https://cycling74.com/forums/importing-from-excel-csv-questions/\">I stole this bit of Max data</a>). From Max, I could visualize the data stream easily using a [multislider] window. I then set out to smooth this data through a variety of techniques. I found that a reliable method of smoothing the noisy data was to create a buffer of the last x number of samples (with [zl.stream]) and then output the mean ([mean]) of that sliding window. The larger the number of samples, the smoother the data, yet this stunts some of the local dynamics within the stream and creates latency. After exploring further I found a 3rd party object [dot.denoise.sliding] that includes better logic for excluding outliers within streams.</p>\n<figure><img src=\"https://jackson.gd/img/breathing-noisey.png\" width=\"20px\" alt=\"Before and after de-noising the stream\"><figcaption>Before and after de-noising the stream</figcaption></figure>\n<p>This returns to an observation I noticed when looking at sensor data. I noticed that one of the gyroscope sensors also was repeating in a rhythmic pattern, but much faster and pronounced than breathing should be. I realized that these were pronounced fulgurations of my heartbeat and that these impulses were actually affecting the accelerometer sensor data I was analyzing from the y-axis.</p>\n<figure><img src=\"https://jackson.gd/img/breathing-gyro.png\" width=\"600px\" alt=\"Notice the fast paced impulses from the gyroscope sensor\"><figcaption>Notice the fast paced impulses from the gyroscope sensor</figcaption></figure>\n<p>Once the oscillations were smoothed, the next step was identifying local minimums and maximums of each breath. Again, the dot library had a nifty object to find local max/mins by checking for changes in sign (+/-) from a previous data point to the next. This would have worked nicely if the data rose and fell with continuity. But due to the noise, there were a number of places near the peak or trough of the oscillations where even a smoothed data point might change the sign of the differences between the current and previous point.</p>\n<p>I tried number of techniques to suppress the improper max/mins that would appear such as forcing a minimum delay to check for a max after finding a min or max, as well as attempting to doubly embed local max/min detection. The most successful and flexible method by far was setting a minimum distance between the breaths that were taken. This is a reliable method as a single breath could vary in speed and the expansion of the torso would be the same.</p>\n<p>I further enhanced this solution by making the distance dynamically shift by an average of the distance between the last five maxes and mins.</p>\n<figure><img src=\"https://jackson.gd/img/breathing-distance.png\" width=\"540px\" alt=\"Gating the frequent max/mins\"><figcaption>Gating the frequent max/mins</figcaption></figure>\n<p>Finally, the last piece of data that is inferred from identifying the local max/mins of each breath is the average time between each inhale and exhale. This is calculated as a metric of respiratory rate consistency which is the difference between the last period between a max/min and the average of these last 10 periods. This value is then scaled and used to attenuate a sound clip of wind chimes that enter when the consistency value is low. Thus, the wind chimes should emerge as a result of a restful cycle of breath and provide positive feedback to the user. In addition to the wind chime samples, there is a three-oscillator drone instrument was driven by the noisy data from the x and z-axis accelerometer streams. This drone adds a subtle resonance to the bells and provides some ambient feedback of the motion of the breath. This is achieved by scaling the smoothed acceleration data from the y-axis and modulating the amplitude.</p>\n<h2 id=\"results-and-conclusions\" tabindex=\"-1\">Results and Conclusions <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-05-20-breathing-through-max/#results-and-conclusions\">#</a></h2>\n<p>After debugging the system for quite some time, I may not be able to provide the best perspective on how useful it is for its intended purpose. Even still, the system may allow a user to pay attention to their biorhythms and less to other distracting thoughts that might interrupt a session. Most of the time spent building the system was in an effort to fine-tune the analysis of these data streams in real-time so many of the acoustic elements were secondary to this objective. The streams were processed flexibly and reliably for the dynamic stream and I’m quite happy with the way the data is handled throughout the system – even in extreme cases such as erratic motion where the amplitude of all sounds is reduced to silence. From this perspective, I find the system quite usable but I am not entirely pleased with the sound of the instruments and sonic environments.</p>\n<p>There are still some unanswered questions I have about more technical aspects of the system. The first being the latency contributed form formatting UDP messages in a Python node and then passing it on to Max. I was fine with this compromise considering the app’s considerable logging speed (100Hz) which enabled much of the noise to be averaged. Another question is one of compatibility: If I were to use another sensor app that was able to directly send OSC data, perhaps at a lower logging rate, would this dramatically affect the system’s ability to identify peaks and troughs? For future work, the system’s reception logic should allow general purpose OSC enabled apps to connect to the system in addition to SensorLog for better compatibility.</p>\n<p>You can find the code <a href=\"https://github.com/jacksongoode/breathingthroughmax\">here</a> along with a setup.</p>\n<figure>\n    <div class=\"iframe-wrapper pb-169\">\n        <iframe src=\"https://drive.google.com/file/d/1G64dQ5iub0O7MClNNkbH5ktVX5hHf0e8/preview\" frameborder=\"0\" allowfullscreen=\"\" alt=\"A demo of the system\"></iframe>\n    </div>\n    <figcaption>A demo of the system using pre-recorded data</figcaption>\n</figure>\n<h1 id=\"works-cited\" tabindex=\"-1\">Works Cited <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-05-20-breathing-through-max/#works-cited\">#</a></h1>\n<p><em>Breathing in Music: Measuring and Marking Time – Finn Upham</em>. <a href=\"https://finnupham.com/2016/10/17/breathing-in-music-making-time-in-music-2016/\">https://finnupham.com/2016/10/17/breathing-in-music-making-time-in-music-2016/</a>. Accessed 27 May 2020.</p>\n<p>Siwiak, Diana, et al. <em>Catch Your Breath - Musical Biofeedback for Breathing Regulation</em>. Audio Engineering Society, 2009. www.aes.org, <a href=\"http://www.aes.org/e-lib/online/browse.cfm?elib=15065\">http://www.aes.org/e-lib/online/browse.cfm?elib=15065</a>.</p>\n<p>Sutarto, Auditya Purwandini, et al. ‘Resonant Breathing Biofeedback Training for Stress Reduction Among Manufacturing Operators’. <em>International Journal of Occupational Safety and Ergonomics</em>, vol. 18, no. 4, Taylor &amp; Francis, Jan. 2012, pp. 549–61. <em>Taylor and Francis+NEJM</em>, doi:10.1080/10803548.2012.11076959.</p>\n",
			"date_published": "2020-05-18T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2020-04-08-soniweb-2/",
			"url": "https://jackson.gd/blog/2020-04-08-soniweb-2/",
			"title": "Soniweb: Epilogue",
			"content_html": "<h1 id=\"a-reworking-of-soniweb\" tabindex=\"-1\">A Reworking of Soniweb <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-04-08-soniweb-2/#a-reworking-of-soniweb\">#</a></h1>\n<p>The <a href=\"https://jackson.gd/soniweb\">Soniweb</a> project was hurridly completed within the span of about a week. However, in rushing through its implementation, there were a number of unique opertunities that would have been possible with the data we were recieving. Notably, the direction of the packets (whether they were being sent or received) was not considered in the process of our sonification. This information would be quite interesting to have conveyed through sound and provide interesting knowledge about when and how often is your computer uploading information to some other server.</p>\n<p>One of the <a href=\"https://youtu.be/Ocq3NeudsVk\">issues</a> I have with sonification is the reasoning (or lack thereof) behind how aspects of data will be sonified. I feel this is a essential point of interpretation artistically that is missed by a lot of sonification projects in the pursuit of making them sonically &quot;beautiful&quot;. Too often the decisions that are being made are not necessarily drawing out acoustic features that parallel the relationships within the data. To this end, I wanted to make three adjustments to the Soniweb project. The <em>first</em> would replace sample playback to synthesized sound at the heart of the sonification, <em>second</em> would focus on shaping the synthesized sound in an extended range of parameters like envelope and pitch based on the features of the incoming packets, and <em>third</em> would be a general optimization of the python script and Pd patch to prevent stuttering or audio glitches during normal use.</p>\n<figure><img src=\"https://jackson.gd/img/soniweb-2-pd.png\" width=\"100%\" alt=\"A little restructured and a much better reverb implementation\"><figcaption>A little restructured and a much better reverb implementation</figcaption></figure>\n<h2 id=\"from-samples-to-fm-synthesis\" tabindex=\"-1\">From samples to FM synthesis <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-04-08-soniweb-2/#from-samples-to-fm-synthesis\">#</a></h2>\n<p>Our first version of Soniweb used pre-fab samples for each of the different protocols (UDP, DNS, TCP, etc.), meaning there was a considerable amount of processing in simply reading these samples into memory to play in parallel. In the previous version, there was noticeable pops and stutters as a result of web browser usage (page loads), Wireshark's backend listening to packets, and Pd producing an ambisonic environment. So, one major reason in replacing samples with a short generated FM synth would be the benefit it might have on CPU usage and memory within Pure Data.</p>\n<h2 id=\"sonification-of-direction\" tabindex=\"-1\">Sonification of direction <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-04-08-soniweb-2/#sonification-of-direction\">#</a></h2>\n<p>It's not totally obvious how one goes about representing motions like approach or retreat. However, one nice point of reference is the <a href=\"https://en.wikipedia.org/wiki/Doppler_effect\">Doppler effect</a>, the reason firetrucks seem to have a higher pitch when driving towards the listener and a lower pitch once they have passed. This might be the best feature representative of acoustic motion along with increasing/decreasing volume.\nÍ</p>\n<figure text-align=\"center\">\n    <audio src=\"https://upload.wikimedia.org/wikipedia/commons/9/90/Speeding-car-horn_doppler_effect_sample.ogg\" controls=\"\" preload=\"none\" class=\"m-0a display-block\"></audio>\n    <figcaption>Audio from Wikipedia</figcaption>\n</figure>\n<p>With this information I planned to give the sense of a sound arriving and departing by creating envelopes that had a short attack/long decay (departing) and long attack/short decay (arriving), each coupled with the proper pitch transformations one would observe with the doppler effect (departing - pitch falling, arriving - pitch rising). However, due to the massive number of packets that are received at once, anything longer than half a second for a single sound would quickly become overwhelming. Thus, a lot of tinkering with the timing and duration of each of these transformations was needed in order to make their different both <strong>intelligible</strong> and <strong>musical</strong>.</p>\n<figure><img src=\"https://jackson.gd/img/soniweb-2-boop.png\" width=\"100%\" alt=\"The new patch for one of the voices\"><figcaption>The new patch for one of the voices</figcaption></figure>\n<h2 id=\"filtering\" tabindex=\"-1\">Filtering <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-04-08-soniweb-2/#filtering\">#</a></h2>\n<p>Because there would be potentially dozen of packets coming within a single second, I decided to restrict this stream by 1) limiting the packet size and the frequency that packets could be registered in the python script and 2) shortening the max duration of the sound each packet will make to 150ms in Pd. These changes came after a number of other tests to see if limiting the packets by duplicate IP's (no packet can be sent from the same IP in succession) and by duplicate packet size. Both of these alternatives seemed like strange modifications to the data stream and were workarounds for my desired outcome - fewer packets at once.</p>\n<p>So, after checking the very scarce documentation of pyshark, I found there was a way to grab the time that a packet was received and restricted the interval between packets manually (5ms). I also made sure that sequential packets of the same size (such as when loading an image) have a subsequently quieter sound.</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"highlight-line\"><span class=\"token comment\"># Set up capture and filter by host IP and packet size</span></span>\n<span class=\"highlight-line\">capture <span class=\"token operator\">=</span> pyshark<span class=\"token punctuation\">.</span>LiveCapture<span class=\"token punctuation\">(</span></span>\n<span class=\"highlight-line\">    interface<span class=\"token operator\">=</span><span class=\"token string\">\"en0\"</span><span class=\"token punctuation\">,</span></span>\n<span class=\"highlight-line\">    only_summaries<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span></span>\n<span class=\"highlight-line\">    bpf_filter<span class=\"token operator\">=</span><span class=\"token string\">\"ip and host \"</span><span class=\"token operator\">+</span>host_ip<span class=\"token operator\">+</span><span class=\"token string\">\" and length > \"</span><span class=\"token operator\">+</span>pkt_len<span class=\"token punctuation\">)</span></span>\n<span class=\"highlight-line\"></span>\n<span class=\"highlight-line\"><span class=\"token keyword\">for</span> packet <span class=\"token keyword\">in</span> capture<span class=\"token punctuation\">:</span></span>\n<span class=\"highlight-line\">        <span class=\"token comment\"># Restrict flow of packets</span></span>\n<span class=\"highlight-line\">        pkt_time <span class=\"token operator\">=</span> <span class=\"token builtin\">float</span><span class=\"token punctuation\">(</span>packet<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">)</span></span>\n<span class=\"highlight-line\">        delta <span class=\"token operator\">=</span> pkt_time <span class=\"token operator\">-</span> prior_time</span>\n<span class=\"highlight-line\">        <span class=\"token keyword\">if</span> delta <span class=\"token operator\">></span> <span class=\"token number\">.05</span><span class=\"token punctuation\">:</span></span>\n<span class=\"highlight-line\">            prior_time <span class=\"token operator\">=</span> pkt_time</span>\n<span class=\"highlight-line\"></span>\n<span class=\"highlight-line\">            <span class=\"token comment\"># To OSC</span></span>\n<span class=\"highlight-line\">            <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span></span>\n<span class=\"highlight-line\">                <span class=\"token comment\"># For reappearing packets of same size - reduce gain</span></span>\n<span class=\"highlight-line\">                <span class=\"token keyword\">if</span> prior_len <span class=\"token operator\">==</span> packet<span class=\"token punctuation\">.</span>length<span class=\"token punctuation\">:</span></span>\n<span class=\"highlight-line\">                    client<span class=\"token punctuation\">.</span>send_message<span class=\"token punctuation\">(</span><span class=\"token string\">\"/packet_len\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>packet<span class=\"token punctuation\">.</span>length<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span></span>\n<span class=\"highlight-line\">                <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span></span>\n<span class=\"highlight-line\">                    client<span class=\"token punctuation\">.</span>send_message<span class=\"token punctuation\">(</span><span class=\"token string\">\"/packet_len\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>packet<span class=\"token punctuation\">.</span>length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></span>\n<span class=\"highlight-line\">                prior_len <span class=\"token operator\">=</span> packet<span class=\"token punctuation\">.</span>length</span></code></pre>\n<figcaption>Filters so my ears (and computer) remain intact</figcaption>\n<p>There was also a helpful flag within pyshark I had overlooked. The &quot;only_summaries&quot; flag when setting up the LiveCapture significantly reduces the information each packet carries when it enters python. Including it likely made the entire script more efficient, as the information I needed from each packet still existed within just the summaries.</p>\n<h2 id=\"results\" tabindex=\"-1\">Results <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-04-08-soniweb-2/#results\">#</a></h2>\n<p>Interestingly, the replacement of sound samples with synthesized sound did not initially reduce the CPU usage of the script on the computer I was testing with (a 2013 Macbook Pro). This may simply be a result of how intensive the setup for testing this program but also that I had made quite extensive changes to the structure of the program as well. Both the script, Pure Data, and a web browser (or web intensive application) are needed to run to &quot;feed&quot; Pure Data packets of data. With all of these applications running, Pure Data does struggle a bit and some pops and stutters when webpages are loading, though I have noticed that this behavior with audio interruptions is also common to other applications like Zoom when there is intense web traffic happening at once.</p>\n<p>After reworking the filtering system and tinkering a bit with the quality of the reverb I added (to help with spatialization), I think I ended up with something quite nice. Here's a short video, please wear headphones to experience the full effect. The code and patches can be found <a href=\"https://github.com/jacksongoode/soniweb\">here</a>.</p>\n<figure><video src=\"https://www.uio.no/english/studies/programmes/mct-master/blog/assets/video/2020_04_13_jacksong_soniweb2_demo.mp4\" width=\"100%\" controls=\"\">Your browser does not support the video tag.</video><figcaption>A demo of the updated project (headphones required!)</figcaption></figure>\n",
			"date_published": "2020-04-08T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2020-03-09-soniweb/",
			"url": "https://jackson.gd/blog/2020-03-09-soniweb/",
			"title": "Soniweb: An Experiment in Web Traffic Sonification and Ambisonics",
			"content_html": "<h1 id=\"sonification-of-the-net\" tabindex=\"-1\">Sonification of the Net <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-03-09-soniweb/#sonification-of-the-net\">#</a></h1>\n<p>For the MCT's sonification course, our group developed a system that collects and geo-locates web requests in real-time <em>and</em> <a href=\"https://en.wikipedia.org/wiki/Sonification\">sonified</a> this incoming data. We were inspired by the paper and project <a href=\"https://www.researchgate.net/publication/334041357_Surfing_In_Sound_Sonification_of_Hidden_Web_Tracking\">Surfing in Sound</a> which sonifies web-trackers via Ableton with an emphasis on user awareness and with potential of extending the experience into an audio-visual installation. A demo of their system can be viewed <a href=\"https://www.youtube.com/watch?v=ug3GfEe801k\">here</a>. Another project with similar aims, <a href=\"https://www.researchgate.net/publication/335452963_Soundbeam_a_Platform_for_Sonifying_Web_Tracking\">Soundbeam</a>, sonifies various trackers through SuperCollider while visiting a web-page as a lens into internet privacy. Our project, Soniweb, finds closer alignment with Surfing in Sound, employing a combination of Wireshark, Python, and Pure Data.</p>\n<p>Our initial plan was to filter network traffic exclusively from the web browser and then analyze the HTTP response headers for information regarding the resources that are loaded when one visits a webpage. From there, we would send that data over OSC to Pure Data, a visual audio programming language and generate sound from it. However, we found (after extensive testing with various programs) that https webpages encrypt the view of the pages' resources (the s stands for the SSL protocol) - this should have been more obvious at the beginning. This prevents a 3rd party application like <a href=\"https://www.wireshark.org/\">Wireshark</a> from spying in on the network activity and reading the data that is transferred. While this is a wonderful standard for the sake of security, it prevented us from going in that direction as Wireshark is unable to decrypt SSL layers. Another application, <a href=\"https://www.telerik.com/fiddler\">Fiddler</a>, an alternative to Wireshark, is able to achieve this, however, its implementation has poor integration with Python and even less on macOS. So our vision broadened to include all possible network activity that is transmitted on a computer and we went with Wireshark as a tried and true stable of the data-sniffing community.</p>\n<h2 id=\"implementation\" tabindex=\"-1\">Implementation <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-03-09-soniweb/#implementation\">#</a></h2>\n<p>We decided early on that it would be quite interesting if we were able to sonify the locations of the data packets that were being sent and received when one uses the computer. From a research oriented position, the sonification and spatialization of web data might provide the computer user with insights into:</p>\n<ol>\n<li>Where the data you see, while casually browsing the web or using a web-dependent application, actually originates from (<strong>digital-physical correspondence &amp; global network monopolies</strong>)</li>\n<li>How one's device is constantly sending and receiving information, even in a seemingly inactive state (<strong>the inextricable internet</strong>) and in contrast...</li>\n<li>How some webpages and applications take much more aggressive approaches in making contact with your device (<strong>web tracking &amp; privacy overreach</strong>)</li>\n</ol>\n<p>These guided our development and led us to tailor the data manipulation towards a sonified experience that could encompass these ideas. This led us to look for databases that would provide a massive list of IP’s with associated geodata (country, city and IP address). We ended up with one of <a href=\"https://dev.maxmind.com/geoip/geoip2/downloadable/\">Geoip2’s</a> free databases, which appeared to be the most comprehensive, free collection of IP-to-coordinate entries available for this task. With their accompanying python package, we were able to import the database quite easily and look up the source and destination IP addresses of every packet that passed through our Wireshark server (tshark) that would be hosted in Python.</p>\n<p>Half of us were first tasked with the development of the network sniffing script in Python. This involved working with a Wireshark wrapper for Python called <a href=\"https://github.com/KimiNewt/pyshark\">pyshark</a> which allows us to host a Wireshark instance and read and filter packet data completely within Python.</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token comment\"># Set up capture and filter by host IP and packet size</span>\ncapture <span class=\"token operator\">=</span> pyshark<span class=\"token punctuation\">.</span>LiveCapture<span class=\"token punctuation\">(</span>interface<span class=\"token operator\">=</span><span class=\"token string\">\"en0\"</span><span class=\"token punctuation\">,</span>\n                              bpf_filter<span class=\"token operator\">=</span><span class=\"token string\">\"host \"</span><span class=\"token operator\">+</span>host_ip<span class=\"token operator\">+</span>\n                              <span class=\"token string\">\"&amp;&amp; length > 60\"</span><span class=\"token punctuation\">)</span></code></pre>\n<p>This snippet of code constructed the capture stream and specified the network we were listening to (the Wi-Fi), which IP addresses we wanted to filter into our stream (our machine's host IP), and what size we wanted the packets to be (this was to reduce the torrents of data that may be less relevant to our network activity).</p>\n<p>There wasn't too much documentation on pyshark’s full capabilities, so we spent a good portion of time getting familiar with the package as well as Wireshark, which the package relied upon. Once we had exhausted the aforementioned possibility of getting metadata from webpages, we began to program a simple packet to OSC script that would send OSC messages to <a href=\"https://puredata.info/\">Pure Data</a> (Pd) when a packet was sent or received into the computer. We would have a Pd patch running simultaneously to the Python script to listen to our OSC messages with the port specified. Here's the code that does this below</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token comment\"># Set up OSC server</span>\nparser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--ip\"</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token string\">\"127.0.0.1\"</span><span class=\"token punctuation\">)</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--port\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">type</span><span class=\"token operator\">=</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token number\">8888</span><span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nclient <span class=\"token operator\">=</span> udp_client<span class=\"token punctuation\">.</span>SimpleUDPClient<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>ip<span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">.</span>port<span class=\"token punctuation\">)</span></code></pre>\n<p>Further development included generating longitude and latitude from the IP addresses and transforming them into ranges that we could eventually use in the Pd patch. We ended up splitting longitudinal values across 8 positions and 4 positions for latitude. This was a compromise due to the computational expense involved using 32 channels from which we would play multiple samples. Because we filtered in packets that either contained our host IP (the IP of the computer running the script) as the source IP or destination IP, we were able to send a single latitude and longitude pair along with a message giving the direction (sent or received). The protocol of the packet (TCP, UDP, TLS, DNS, other) and its length (or size) was also sent as OSC messages.</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token comment\"># Sent</span>\n<span class=\"token keyword\">if</span> packet<span class=\"token punctuation\">.</span>ip<span class=\"token punctuation\">.</span>src <span class=\"token operator\">==</span> host_ip<span class=\"token punctuation\">:</span>\n    dst_resp <span class=\"token operator\">=</span> reader<span class=\"token punctuation\">.</span>city<span class=\"token punctuation\">(</span>packet<span class=\"token punctuation\">.</span>ip<span class=\"token punctuation\">.</span>dst<span class=\"token punctuation\">)</span>\n    client<span class=\"token punctuation\">.</span>send_message<span class=\"token punctuation\">(</span><span class=\"token string\">\"/dest_name\"</span><span class=\"token punctuation\">,</span> dst_resp<span class=\"token punctuation\">.</span>country<span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">)</span>\n    client<span class=\"token punctuation\">.</span>send_message<span class=\"token punctuation\">(</span><span class=\"token string\">\"/direction\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n    client<span class=\"token punctuation\">.</span>send_message<span class=\"token punctuation\">(</span><span class=\"token string\">\"/ip_lat\"</span><span class=\"token punctuation\">,</span>\n                        <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>dst_resp<span class=\"token punctuation\">.</span>location<span class=\"token punctuation\">.</span>latitude<span class=\"token operator\">+</span><span class=\"token number\">90</span><span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">60</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n                        <span class=\"token comment\"># 0 to 3 (4 degrees of differentiation)</span>\n    client<span class=\"token punctuation\">.</span>send_message<span class=\"token punctuation\">(</span><span class=\"token string\">\"/ip_long\"</span><span class=\"token punctuation\">,</span>\n                        <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>dst_resp<span class=\"token punctuation\">.</span>location<span class=\"token punctuation\">.</span>longitude<span class=\"token operator\">+</span><span class=\"token number\">180</span><span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">51.43</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n                        <span class=\"token comment\"># 0 to 7 (8 degrees of differentiation)</span>\n\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Sent to \"</span><span class=\"token operator\">+</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>dst_resp<span class=\"token punctuation\">.</span>country<span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre>\n<h2 id=\"sonification\" tabindex=\"-1\">Sonification <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-03-09-soniweb/#sonification\">#</a></h2>\n<p>All the while, our third group member started development on the Pure Data patch simultaneous to our work in Python. He began with a sample handler that would play five different samples that corresponded to the protocol type, amplitude. We initially attempted ambisonics production through <a href=\"https://www.reaper.fm/\">Reaper</a> as a DAW but found a wonderful (and actively developed updated) library for Pd called <a href=\"https://github.com/Spacechild1/vstplugin\">vstplugin~</a> where we loaded the <a href=\"https://plugins.iem.at/\">IEM</a> plugin suite for ambisonics. The library worked quite well for our purposes and we were able to load each of our 32 channel’s azimuth and elevations as an initialization process. Each of the “voice” sub-patches contain five “samplehandler” sub-patches to play any of the potential five sounds given the protocol message. Within these sub-patches is one “singlesample” sub-patch that plays the sample with “tabread4~”. However, this sub-patch is cloned for times for the possibility of a single channel playing a particular sample up to four times simultaneously. This was built in due to the high frequency of packet data that we receive during the python script’s playback.</p>\n<figure><img src=\"https://jackson.gd/img/soniweb-pd.png\" width=\"100%\" alt=\"The Pure Data patch (and sub-patches)\"><figcaption>The Pure Data patch (and sub-patches)</figcaption></figure>\n<h2 id=\"the-effect\" tabindex=\"-1\">The Effect <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-03-09-soniweb/#the-effect\">#</a></h2>\n<p>When running the script and patch simultaneously, the network activity that your current machine is relaying is sonified and located in a <a href=\"https://en.wikipedia.org/wiki/Ambisonics\">5th order ambisonics</a> environment. The various bells ping around the listener as if they were sitting in the center of the Earth listening to the machines around them speak to one another. While we achieved our set goals, there are a few unanswered questions as to whether the packets that are being received are actually too quick to receive and process in real-time and may become backlogged over the course of a session. This may either have to do with limitations imposed by Pd, Python, Wireshark or the CPU power afforded by the user’s computer. In a recent update, converting one of the &quot;if&quot; statements to a dpf filter within pyshark's instantiation made the incoming packets quite responsive to web browsing activity. However, it may be possible to mitigate these issues and enhance the sonification by generating the instruments within Pd rather than using sample files. This project has great potential for an extension into a live installation, one, perhaps, that asks the audience to connect their devices to a network hosted in the space so that they could listen to the network activity of their collective devices.</p>\n<figure><video src=\"https://www.uio.no/english/studies/programmes/mct-master/blog/assets/video/2020_03_07_jacksong_soniweb_demo.mp4\" width=\"100%\" controls=\"\">Your browser does not support the video tag.</video><figcaption>A demo of Soniweb (headphones required!)</figcaption></figure>\n<h1 id=\"works-cited\" tabindex=\"-1\">Works Cited <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-03-09-soniweb/#works-cited\">#</a></h1>\n<p>“Fiddler - Free Web Debugging Proxy - Telerik.” <em>Telerik.com</em>, <a href=\"https://www.telerik.com/fiddler\">https://www.telerik.com/fiddler</a>. Accessed 19 Apr. 2020.</p>\n<p>Hutchins, Charles, et al. <em>Soundbeam: A Platform for Sonifying Web Tracking</em>. 2014.</p>\n<p><em>GeoIP2 Downloadable Databases « MaxMind Developer Site</em>. <a href=\"https://dev.maxmind.com/geoip/geoip2/downloadable/\">https://dev.maxmind.com/geoip/geoip2/downloadable/</a>. Accessed 19 Apr. 2020.</p>\n<p>Green, Dor. <em>KimiNewt/Pyshark</em>. 2013. 2020. GitHub, <a href=\"https://github.com/KimiNewt/pyshark\">https://github.com/KimiNewt/pyshark</a>.</p>\n<p>Lutz, Otto, et al. <em>Surfing In Sound: Sonification of Hidden Web Tracking</em>. 2019. ResearchGate, doi:10.21785/icad2019.071.</p>\n<p><em>Pure Data — Pd Community Site</em>. <a href=\"https://puredata.info/\">https://puredata.info/</a>. Accessed 19 Apr. 2020.</p>\n<p><em>REAPER — Audio Production Without Limits</em>. <a href=\"https://www.reaper.fm/\">https://www.reaper.fm/</a>. Accessed 19 Apr. 2020.</p>\n<p>Ressi, Christof. <em>Spacechild1/Vstplugin</em>. 2019. 2020. GitHub, <a href=\"https://github.com/Spacechild1/vstplugin\">https://github.com/Spacechild1/vstplugin</a>.</p>\n<p>Rudrich, Daniel. <em>IEM Plug-in Suite. plugins.iem.at</em>, <a href=\"https://plugins.iem.at/\">https://plugins.iem.at/</a>. Accessed 19 Apr. 2020.</p>\n<p><em>Wireshark · Go Deep</em>. <a href=\"https://www.wireshark.org/\">https://www.wireshark.org/</a>. Accessed 19 Apr. 2020.</p>\n",
			"date_published": "2020-03-09T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2020-02-10-osc-guitar/",
			"url": "https://jackson.gd/blog/2020-02-10-osc-guitar/",
			"title": "Strumming through space and OSC",
			"content_html": "<h1 id=\"an-osc-guitar\" tabindex=\"-1\">An OSC Guitar <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-02-10-osc-guitar/#an-osc-guitar\">#</a></h1>\n<p>My second project for the audio programming course was to accurately simulate a guitar strum in <a href=\"https://puredata.info/\">Pure Data</a> (Pd) using the sensors on a mobile device. I wanted its interaction to be reflective of the direction, speed, and acoustic uniqueness of a real guitar strum. By uniqueness I mean the qualities of a strum that are not necessarily intended by the musician, like the intensity of each string strike and the delay between each individual string as the fingers (or pick) slide across the strings. These additions make both the sound and the experience of strumming quite realistic and, as implemented in this patch, benefit the realism of the virtual strum. However, a number of unforeseen difficulties made the feat of a seamless gesture-to-sound production a challenge. As I will discuss, one of the greatest barriers to responsiveness was network latency and the inability for native, local interfacing within both Pure Data and sensor data on my mobile device.</p>\n<figure><img src=\"https://jackson.gd/img/oscguitar-g-main.png\" width=\"100%\" alt=\"Frontpage of Pd patch\"><figcaption>Frontpage of Pd patch</figcaption></figure>\n<h2 id=\"strings-and-things\" tabindex=\"-1\">Strings and Things <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-02-10-osc-guitar/#strings-and-things\">#</a></h2>\n<p>I began the project with a detailed search for an accurate model of a string. While I could have built a simple string, the focus of my work was integrating the string model into a framework from which it could serve as a guitar. The string was built using a digital waveguide model by <a href=\"https://ccrma.stanford.edu/realsimple/waveguideintro/\">Edgar J. Berdahl and Julius O. Smith</a>. Their model is quite good, both to the ear and to the standards of Stanford's Department of Music.</p>\n<figure><img src=\"https://jackson.gd/img/oscguitar-g-waveguide.png\" width=\"100%\" alt=\"Waveguide model of a string\"><figcaption>Waveguide model of a string</figcaption></figure>\n<p>However, the model was not designed in a way to be used in polyphony. To correct their design, I renamed each array, send, and receive to be unique for each instance of the waveguide model using a prefix of &quot;$0-&quot;. This allowed each object to be instantiated with a unique identity and as a result, enabled the messages passed by each of the six strings to exist without overwriting potential shared arrays or variables. I also reformatted some of their code to better fit my purposes like adding inlets and outlets that would communicate with the main patch and the six strings in concert. I also cleaned and reorganized their layouts to make more functional sense (it was messy, sorry Stanford).</p>\n<figure><img src=\"https://jackson.gd/img/oscguitar-g-model.png\" width=\"100%\" alt=\"The guitar model sub-patch\"><figcaption>The guitar model sub-patch</figcaption></figure>\n<p>Once the strings were in place, I assigned the fundamental frequency of each string of four chords to four messages and made a simple metronome to send a chord pattern to play the chords (for a non-interactive demo). To create a delay between each string I used the &quot;pipe&quot; object prior to reaching the waveguide model. The delays' right-hand argument allows for an input number, whose sign (positive of negative) determines whether the string delays will cascade downwards or upwards. The expression also considers randomness by calling six variables that were assigned six random numbers included in the sub-patch &quot;rnd-strings&quot;.</p>\n<figure><img src=\"https://jackson.gd/img/oscguitar-rnd-strings.png\" width=\"100%\" alt=\"Sub-patch for the randomizer\"><figcaption>Sub-patch for the randomizer</figcaption></figure>\n<p>These random variables are created on every &quot;strum-bang&quot;, a global variable used to send a bang when there is a strum (either from the metro or OSC). The random variables in the delay between each string pluck increase or decrease the total delay time. This delay between each string in a strum is also factored by the acceleration of the strum). After the audio signals are generated, they have their amplitude reduced by the same random factors used before. There is then some light panning of the mono signals of the top two and bottom two strings. This is finally sent to the dac.</p>\n<h2 id=\"the-notorious-osc\" tabindex=\"-1\">The Notorious OSC <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-02-10-osc-guitar/#the-notorious-osc\">#</a></h2>\n<figure><img src=\"https://sensors2.org/wp-content/uploads/2015/02/osc.png\" width=\"100%\" alt=\"Sensor2OSC\"><figcaption>Sensor2OSC</figcaption></figure>\n<p>The last step was into employ a method to send <a href=\"https://en.wikipedia.org/wiki/Open_Sound_Control\">OSC</a> messages, generated from my smartphone's sensors, to the patch running on my computer. I used the Android app <a href=\"https://github.com/SensorApps/Sensors2OSC\">Sensors2OSC</a> to send OSC messages from the gravity and linear acceleration sensors in the phone. I assigned the IP and port within the Android app to which the OSC messages would be sent and then assigned the same port within the patch to receive these messages over the local network (though it could be potentially on any network). For sensor data, I chose the gravity sensor over the accelerometer sensor because the tilt values that correspond to rotating the top of the phone downwards and upwards were distinctly positive and negative as the top the crossed over the y-axis (horizon).</p>\n<figure><img src=\"https://jackson.gd/img/oscguitar-g-osc.png\" width=\"100%\" alt=\"Receiving and manipulating OSC data from the phone\"><figcaption>Receiving and manipulating OSC data from the phone</figcaption></figure>\n<p>Two mechanisms were created to trigger a bang from a strum using the phone. The first opens or closs a gate depending on whether or not there is a sufficient change in the tilt of the top of the phone from the gravity sensor. The second mechanism determines if the acceleration of the strum is enough to trigger a bang. In its design, the second impulse generator can only be activated if there is enough of a properly shaped strum gesture. In addition to the thresholds preventing any rapid activation, I also created a refractory period that prevents a bang from being sent until the previous strum has completed from calculating strum duration. The impulse that is sent when a strum makes it through the thresholds is a positive value from the linear acceleration. The sign of this number is then changed depending on whether the strum was down or up (using the sign of the gravity sensor's change). This number determines the speed and direction of the strum. If all thresholds were tuned perfectly and there was zero latency (my next point), this would allow rapid strumming of the virtual guitar.</p>\n<h2 id=\"complications\" tabindex=\"-1\">Complications <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-02-10-osc-guitar/#complications\">#</a></h2>\n<p>Unfortunately, one of the first issues I recognized was the latency that appeared from communicating over the local network. The latency was also inconsistent as well as the message rate that was being received (not to mention messages dropped). There were some moments where the speed and response of the OSC messages was incredibly fast and at other times inoperably slow. Because of this, a good portion of my time was spent finding threshold values and averages that catered to the message rate and latency. There is also the issue of this configuration being device dependent. Different smartphones will likely have different sensor hardware, sensor names, ranges, rates, etc. so this patch is certain to need some configuring in each case. The initial goal of this project was to build the app in Pd and afterwards &quot;simply and easily&quot; port it over to the <a href=\"https://danieliglesia.com/mobmuplat/\">MobMuPlat</a> application for Android and iOS. However, this app does not appear to read the necessary sensor data natively and there do not appear to be alternatives in hosting Pd patches on Android. For the time being, this patch will have to be hosted on a remote computer and receive OSC messages from a device connected to the same network.</p>\n<figure><video src=\"https://jackson.gd/blog/2020-02-10-osc-guitar/vid/osc-guitar.mp4\" width=\"100%\" controls=\"\">Your browser does not support the video tag.</video><figcaption>Video demo of the OSC functionality - credit: Thomas Anda</figcaption></figure>\n<p>The audio within the video was recorded internally (oddly the pops only appeared after video editing - my computer was struggling with that fabulous transition).</p>\n<p>All in all, this project was a great introduction to Pure Data and definitely taught us the general principles of the language as well as some of the challenges you only encounter in hour 11 of debugging. While making this a standalone app would require a bit more investigation into MobMuPlat or some other Pd host-able interface, I think my goals were met. And when the network connection is good and the thresholds are polished, it's quite suprising how responsive a network connection can be.</p>\n<p>My project's code can be found <a href=\"https://github.com/jacksongoode/osc-guitar\">here</a>.</p>\n<h2 id=\"works-cited\" tabindex=\"-1\">Works Cited <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2020-02-10-osc-guitar/#works-cited\">#</a></h2>\n<p>Iglesia, Daniel. <em>Monkeyswarm/MobMuPlat</em>. 2013. 2020. GitHub, <a href=\"https://github.com/monkeyswarm/MobMuPlat\">https://github.com/monkeyswarm/MobMuPlat</a>.</p>\n<p><em>MobMuPlat - Mobile Music Platform</em>. <a href=\"https://danieliglesia.com/mobmuplat/\">https://danieliglesia.com/mobmuplat/</a>. Accessed 7 Feb. 2020.</p>\n<p><em>Plucked String Digital Waveguide Model</em>. <a href=\"https://ccrma.stanford.edu/realsimple/waveguideintro/\">https://ccrma.stanford.edu/realsimple/waveguideintro/</a>. Accessed 7 Feb. 2020.</p>\n<p><em>SensorApps/Sensors2OSC</em>. 2014. SensorApps, 2020. GitHub, <a href=\"https://github.com/SensorApps/Sensors2OSC\">https://github.com/SensorApps/Sensors2OSC</a>.</p>\n<p><em>Sensors2OSC - Sensors2</em>. <a href=\"https://sensors2.org/osc/\">https://sensors2.org/osc/</a>. Accessed 7 Feb. 2020.</p>\n",
			"date_published": "2020-02-10T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2019-10-01-memory-avalanches/",
			"url": "https://jackson.gd/blog/2019-10-01-memory-avalanches/",
			"title": "Memory and Modality through The Avalanches&#39; Wildflower and &quot;The Was&quot;",
			"content_html": "<figure><img src=\"https://jackson.gd/img/avalanches.webp\" width=\"100%\" alt=\"The Avalanches in the early 2000's\"><figcaption>The Avalanches in the early 2000's</figcaption></figure>\n<p>For many, music leaves some of the deepest impressions we have during our lifetime. One's favorite song might often trigger a flashback, with a sudden return to those past moments they shared with the sound. These returns are accompanied with a flood of detailed sensory information that spread across our senses in what might be called an <em>episodic memory</em>. As we can appreciate in these moments, our memories are wrapped in complex, multi-modal associations that are encoded within our hippocampus, thalamus, amygdala, and various areas of our cortex. Thus, the act of recalling episodic memories stimulates these neural pathways just as they were activated when we first consolidated the memory. Björn Vickhoff explains the process wonderfully in his <em>A Perspective Theory of Music Perception and Emotion</em>,</p>\n<blockquote>\n<p>&quot;Even if the information is integrated in the hippocampus and the hippocampus is associated with long-term memory this does not mean that all memories are stored in the hippocampus. Rather the information is fed back to the perception and association areas. Episodic memorizing is to reactivate the perception process. This is what makes episodic memories so vivid. The process can be activated by any contextual cue. Since we live the event anew, this is by definition imagery. I believe that imagery and episodic memory is connected&quot; (Vickhoff 2008).</p>\n</blockquote>\n<p>Something as simple as the almost tangible link between the smell of freshly baked cookies and one's grandmother is a product of the widely encoded information integrated upon each of our experiences, especially those experienced with strong emotion or personal meaning. Stefan Koelch describes this musicogenic meaning of musical &quot;relations comprise memory representations… as well as associations with regard to an individual's most personal inner experiences&quot; (Koelch, 2011). I would like to explore this phenomenon through the lens of music cognition, focusing chiefly on how our modalities contribute to an experience of music that can transport us through memory and nostalgia.</p>\n<figure>\n\t\t\t\t<lite-youtube videoid=\"MFEZiMyfSYI\">\n\t\t\t\t\t<button type=\"button\" class=\"lty-playbtn\">\n\t\t\t\t\t\t<span class=\"lyt-visually-hidden\">Every Sample From The Avalanches' Since I Left You</span>\n\t\t\t\t\t</button>\n\t\t\t\t</lite-youtube>\n\t\t\t\t<figcaption>Every Sample From The Avalanches' Since I Left You</figcaption>\n\t\t\t</figure>\n<p>The Avalanches' <em>Since I Left You</em> is an album composed as a collage of thousands (~3500) of samples of records, movies, tv shows, and advertisements before it. This genre of music takes many names, but aligns within the traditions of plunderphonics. For The Avalanches, the absence of original musical material makes their record absolutely drenched in cue, reference, and signposts from the past. Their employment of appropriation, while frowned upon by musical history as a whole, allows their music to reach into the past and re-contextualize the samples they use within new frames of reference. While one could write an entire essay on these transformations alone, the effect of this method of production provides a massive launchpad from which we can dive into memory and nostalgia, both personal and cultural. However, the effect of this experience is, incredibly, a product of only our listening; how, then, would our experience change once we integrate a visual element such as that of an accompanying video?</p>\n<figure>\n\t\t\t\t<lite-youtube videoid=\"3VN93T7m0-U\">\n\t\t\t\t\t<button type=\"button\" class=\"lty-playbtn\">\n\t\t\t\t\t\t<span class=\"lyt-visually-hidden\">The Was, Soda_Jerk</span>\n\t\t\t\t\t</button>\n\t\t\t\t</lite-youtube>\n\t\t\t\t<figcaption>The Was, Soda_Jerk</figcaption>\n\t\t\t</figure>\n<p>For the release of their second album, <em>Wildflower,</em> The Avalanches collaborated with video artist Soda_Jerk to produce a music video well suited for plunderphonics. &quot;The Was&quot; is an audiovisual piece that mimics the stylistic composition of The Avalanches' audio production in visual form. Set to a mix of songs from the album, &quot;The Was&quot; samples from films, music videos, TV and cartoons of the past in an effort to visualize the sonic collage of the past The Avalanches managed to build. The multi-sensory experience of enjoying &quot;The Was&quot; is a journey through the cultural iconography of the past and, as such, 1) directs the listener to marry the sound of _Wildflower* to the visual media of the past, 2) invites them to revisit memories of these audiovisual relics and the atmospheres of sentiment that are evoked, and 3) forms new meaningful associations between these samples and their personal memories, neurally and semantically.</p>\n<figure>\n    <img width=\"300\" height=\"300\" src=\"https://is1-ssl.mzstatic.com/image/thumb/Music128/v4/43/b3/7e/43b37e75-6027-0d59-5f76-5a68219c82e3/source/600x600bb.jpg\">\n    <figcaption>Wildflower, The Avalanches</figcaption>\n</figure>\n<p>Like their past works, <em>Wildflower</em> uses found sound and sonic elements from unauthored music evoke a sense of nostalgia for a period across which their samples are spread. 16 years after the release of their seminal album, <em>Since I Left You</em>, The Avalanches made a return to the music scene with a new record, that continued their re-contextualization of sound that might be a little more in touch with today. While the album consists primarily of found and sampled sound, some tracks feature collaborations with contemporary artists that contribute new vocal or instrumental material. When one listens to <em>Wildflower</em>, they can hear the vintage quality of these samples, many of which have been lifted from wax. Mixing and mastering techniques and styles emblematic of their respective decade imbue these samples with their recording period, even as they have been chopped, processed, and reoriented within a new mix. Listening to &quot;Sunshine&quot; or &quot;Harmony&quot; one can hear vocal and string arrangements of the late 60's, early 70's with that characteristic lush quality diminished high end, reminding one of the easy-listening, blue-eyed soul period of the era. It might bring to mind the Summer of Love and a collection of artists (The Mamas and Papas, The Beach Boys, The Monkees etc.) that saturated the era.</p>\n<figure>\n    <img width=\"400\" src=\"https://images.squarespace-cdn.com/content/561c0052e4b0faa1a2071248/1486512095702-AMS4W285ORMY5M71KOF8/00864-Summer-of-Love-1967-Travels-with-Gloria-travelswithgloria.wordpress.com-pic.jpg?format=1500w&content-type=image%2Fjpeg\">\n    <figcaption>The Summer of Love, <a href=\"https://www.visitfishermanswharf.com/events/summeroflove\">Fisherman's Wharf</a></figcaption>\n</figure>\n<p>All of these auditory cues may very well stimulate episodic memories, the phenomenon that Davies calls the &quot;Darling, They're Playing Our Tune&quot; (DTPOT) theory of musical response (Davies, 1978). Something like Hal Blaine's <a href=\"https://musicforants.com/post/123994305022/be-my-baby-drum-beat-mix\">iconic drum intro</a> into &quot;Be My Baby&quot; is immediately recognizable for many and likely there is a strong emotional response from those who are familiar of the song and the era. While direct reference to exact sonic material may not be recognizable for many within The Avalanches densely packed quilts of samples, it is the textural quality of this vintage sound, transposed through time, that is central to our experience of nostalgia. Admittedly, this task of sending listeners back into the past is might not be accessible for many without the musical history or associations. Thus, many musicians create video projects to supplement the artwork through a new sensory experience.</p>\n<p>&quot;The Was&quot; splits <em>Wildflower</em> open and allows us to understand the work within a different modality that achieves a parallel effect, employing appropriation and composition to produce a video collage of cultural iconography. Film may be one of these mediums where we contextualize music and develop an understanding of what this past was emblematic of (even if it wasn't experienced).</p>\n<p>Soda_Jerk deftly builds a visual story through the arrangement of 171 films, many of them situated as (cult) classics of counter culture in their respective era. Scenes from media of the past unfold like a travel log set to a mix of <em>Wildflower</em>. In this way, the multi-sensory experience provides an even greater wealth of signs from which we may experience our memories from the past once more. But what's new new now, is the experience of having these scattered audio and visual exchanges, dating as far back as half a century ago, enter into dialogue with one another.</p>\n<p>The conversation that exists when watching &quot;The Was&quot; connects the information we are receiving through both modalities, neurally and symbolically, the result spread across our memory systems. Considering the research suggesting that the consolidation and retrieval of memory is inherently bound within the cognitive structures like the hippocampus and cortex, one can imagine that their pre-existing memories recalled by the music and filmography of &quot;The Was&quot; are interwoven, encompassing now hundreds of audiovisual points of contact. After a viewing, the audience has been carried through time, visually, aurally, and perhaps emotionally and experientially, through memory.</p>\n<p><em>Wildflower</em> and &quot;The Was&quot; exemplify the power of re-contextualization as an apparatus of art making and the cognitive mechanisms able to form and reform rich personal memories. Experienced as a single modality, music provides more than enough sensory information (especially for musicians) that can drive us back to the moments and sentiments we've shared with the sound over our life. But once we add a secondary sensory experience, one as stimulating as video, we face a cross-modal event. Music videos and audiovisual projects alike arrange complex and disruptive encounters that seek to reorder and enhance the multimedia experience through our own involvement in the assimilation of audio/visual stimuli and sentimentality.</p>\n<h4 id=\"works-cited\" tabindex=\"-1\">Works Cited <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2019-10-01-memory-avalanches/#works-cited\">#</a></h4>\n<p>Davies, John Booth. <em>The Psychology of Music</em>. Stanford University Press, 1978.</p>\n<p>Groussard, Mathilde, et al. &quot;When Music and Long-Term Memory Interact: Effects of Musical Expertise on Functional and Structural Plasticity in the Hippocampus.&quot; <em>PLOS ONE</em>, vol. 5, no. 10, Oct. 2010, p. e13225. <em>PLoS Journals</em>, doi:10.1371/journal.pone.0013225.</p>\n<p>Koelsch, Stefan. &quot;Toward a Neural Basis of Music Perception – A Review and Updated Model.&quot; <em>Frontiers in Psychology</em>, vol. 2, 2011. <em>Frontiers</em>, doi:10.3389/fpsyg.2011.00110.</p>\n<p>Koelsch, Stefan. &quot;Towards a Neural Basis of Processing Musical Semantics.&quot; <em>Physics of Life Reviews</em>, vol. 8, no. 2, June 2011, pp. 89–105. <em>ScienceDirect</em>, doi:10.1016/j.plrev.2011.04.004.</p>\n<p><em>Since I Left You</em>. Elektra / Wea, 2001.</p>\n<p>Snyder, Bob. <em>Music and Memory: An Introduction</em>. MIT Press, 2001.</p>\n<p><em>The Was (2016)</em>. <em>YouTube</em>, <a href=\"https://www.youtube.com/watch?v=3VN93T7m0-U\">https://www.youtube.com/watch?v=3VN93T7m0-U</a>. Accessed 4 Oct. 2019.</p>\n<p>Vickhoff, Björn. <em>A Perspective Theory of Music Perception and Emotion</em>. 2008. <em>gupea.ub.gu.se</em>, <a href=\"https://gupea.ub.gu.se/handle/2077/9604\">https://gupea.ub.gu.se/handle/2077/9604</a>.</p>\n<p><em>Wildflower</em>. Astralwerks, 8 July 2016.</p>\n",
			"date_published": "2019-10-01T00:00:00Z"
		}
		,
		{
			"id": "https://jackson.gd/blog/2018-11-02-43-rpm/",
			"url": "https://jackson.gd/blog/2018-11-02-43-rpm/",
			"title": "43 rpm",
			"content_html": "<h3 id=\"title-card\" tabindex=\"-1\">Title Card <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#title-card\">#</a></h3>\n<blockquote>\n<p>“Indeed a lake is within me, solitary and self-contained;\nbut the river of my love draws it off-down to the sea!”</p>\n</blockquote>\n<p>Cut from black. A shot of the sky in silence. Branches of trees are tugged by the wind and framed in a blue tarp, half-filled with clouds. After a few moments, inches above the grassy ground is the top half of Clover on her back, watching the sky. A thick head of opal brown hair, filled with shades of red, is spread over the grass and over an oversized white t-shirt she’s wearing. Clover’s face is freckled and pale. Hers eyes are set neatly under her thick eyebrows, and have low rings under them, making her whole gaze look deeply fatigued but at peace. Sound of those heavy branches in the wind fade in, and after half a minute, the shot cuts to the same angle from looking up at ceiling of her room, light blue with a ceiling fan slowly rotating.</p>\n<h3 id=\"room\" tabindex=\"-1\">Room <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#room\">#</a></h3>\n<p>A woman in her late forties, Diane, is seen sitting on a chair in the corner of Clover’s dorm, peacefully watching her sleep. In this moment, the sound of the wind, which has persisted through the transition, begins to shift to the ring of a radiator and the ruffling of sheets can be heard behind the shot. It’s around 9am and soft light pours in from an adjacent window. A number of angles from Clover’s room are shown; her dresser below the window, jackets and leggings retreated in corners are Diane is gone now. From a desk at the height of her bed, Clover is beginning to wake up but has rolled over to face the wall bordering her bed. Her hair is spread out over her pillow, which is a light blue, like the sheets. A cell phone rings, vibrating the sheets and lighting up the wall where she’s curled up. The shot moves to her face where a fly has landed on her cheek.</p>\n<h3 id=\"office\" tabindex=\"-1\">Office <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#office\">#</a></h3>\n<p>As she weakly swipes it away, the scene cuts to Clover now sits in a warmly lit office, it’s mid-afternoon and Clover’s sitting in a wide blue sofa, a dark green button down and jeans, with one leg tucked under her. The area is clean with lots of windows, a desk, sofa, and two chairs. Diane sits in a chair opposing Clover. Diane has a posture that seems to have grown only more elegant in age. She wears a gray cardigan too big for her shoulders slouches over her thin frame. She has short graying hair, thin glasses, and the lean face of an athlete or someone the sun had gifted with agelessness. She keeps a pen and pad by her side, looking carefully at Clover’s withdrawn daze. On the walls are various generic landscapes that were most likely picked up at yard-sales.</p>\n<p><strong>Diane:</strong> &quot;Well, its good to see you again.&quot; A small smile forms from a pair of even smaller lips.</p>\n<p><strong>Clover:</strong> &quot;You too” And she turns to face Diane, flashing a smile.</p>\n<p><strong>D:</strong> “So, how have things been going this week?”</p>\n<p><strong>C:</strong> “They've been good. I think things are getting better, but you know you can never be sure right away.” She pauses “But, yeah, definitely better than last week.”</p>\n<p><strong>D:</strong> “Right, well… good!” She take a moment to look at Clover once more. Her gaze falls to the note pad. “And it's all about progress, you know, one week at a time. Some weeks might be better, like this one, others worse and as long as we’re working on it, we’ll be exploring more and more solutions.”</p>\n<p><strong>C:</strong> “Yeah,&quot; Clover responds, looking more earnest than she believes.</p>\n<p><strong>D:</strong> “So, how have you been able to sleep recently since our last suggestions, with writing and-?”</p>\n<p><strong>C:</strong> “Yeah, yeah, it’s better, it's hard to make time for it some nights. But its been fun taking my mind off of things before bed. It's still tough since, eventually, I'll have to decide to quit writing and try sleeping&quot; she pauses &quot;and that's when I start to struggle again. I wish I could write myself to sleep, but my hand gets tired faster than my brain.&quot; She continues, &quot;My teacher told me almonds could help too?&quot;</p>\n<p><strong>D:</strong> “Yeah, I’ve heard that too, some nutritionists believe a handful might help some people before bed, though I'm not sure why, so don't take my word for it&quot; she chuckles but becomes more serious. &quot;But I’m glad you’ve been trying out writing. If I may ask, what kinds of things do you usually write about?”</p>\n<p><strong>C:</strong> “Oh, um, its mostly just my imagination.” Clover brings her hands to rub her neck. She looks uncomfortable, almost guilty, as she thinks about her answer.</p>\n<p>After she finishes, the camera stays on her, listening to Diane. The sound of her voice drifts (pans left) and fades into the distance, the buzzing of a fly intensifies (panned right). She gets distracted and watches the insect fly upwards towards Clover’s ceiling fan, in a cut back to her bedroom.</p>\n<h3 id=\"room-1\" tabindex=\"-1\">Room <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#room-1\">#</a></h3>\n<p>Clover is groggily staring at her phone. After a few seconds of reading. she frantically shakes out of bed, her long brown hair swings with her, as she hops into jeans, sneakers, a sweater and grabs her keys from the top of a dresser and slams the door. The fly comes to rest on her blanket, which has now settled on the hardwood floor, among other scattered miscellanies. Her running feet are heard outside along the hall for a moment, soon, out of audible range. From the inside of the dorm-rooms, Clover’s feet are seen sprinting past a series of doorways. The sound of her stomps is now being followed, along with heavy breath, from one hall to the next, on and off carpet and wood and finally down a stairwell. Her feet are followed from behind, as the run down the last hall and are trailed through the exit.</p>\n<p>A heavy wooden door of the dorm bursts open with the sound of birds chirping in the early morning. Clover looks around the campus as if expecting to find the source of her stress, still panting from her run through the halls. She slowly leans over, her hands on her knees, breathing erratically, trying to find a natural pace of breath again with whispered counts upwards from “one”. The campus is empty, and a wide shot puts Clover in a yard where other tall and thin dormitories are sat side by side like studious guardians. Suddenly, the sound of her name being shouted has grown in volume, enough to break through her breaths. Clover stops breathing and quickly turns her head towards the shouts</p>\n<h3 id=\"lot\" tabindex=\"-1\">Lot <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#lot\">#</a></h3>\n<p>Her turn cuts the shot to an empty parking lot at night. She’s now sitting, staring up at a florescent light, coming from a single streetlamp. She’s in different clothes, a heather gray sweatshirt and gym shorts, she sits between the headlights of a car, her back resting against the front bumper. The electric buzz of the lamp rolls in and behind her the muffled yelling she had heard before returns more clearly; it’s a muffled male voice, angrily shouting her name but goes quiet moments later. She reaches her arms over to cover the headlights on either side of her, creating red outlines around her hand and darkening her face. There’s nothing visible beyond what’s lit by the street lamp and the car headlights and in fact, the parking lot looks a lot like an island suspended in the void. The asphalt is cracked and ribbed from the movement of earth underneath it. The light hum of a small engine enters soon after. The scene cuts to Clover walking away from the car. A door behind her opens and the car door bell begins ringing. Christ steps out of the car.</p>\n<p><strong>Chris:</strong> “Clove! Where are you going!?” An irritated voice behind her yells “Why are you doing this? We can talk in the car - Clove! - what is wrong with you?” Clover turns around.</p>\n<p><strong>Clover</strong>: “Do you fucking think it’s okay for you to act that way around my family?” Clover’s yelling. Her voice strains from holding back tears and so it comes out in bursts. While she says this her eyes dark around the silhouette of the figure, partially blinded by the headlines she’s standing in front of. ”How fucking old are you? I’ve told you how much it hurts him - can’t you be fucking respectful- why can’t you - just apologize for once!?”</p>\n<p><strong>Ch</strong>: “What? Was I supposed to not say something while your brother just starts screaming at the top of his lungs at 2am? How was I supposed to know he had a thing? He was screaming all weekend.” Chris’s voice is just as loud but less uncertain. Clover walks backwards from the headlights.</p>\n<p><strong>C:</strong> “Go home. I don’t want to see you. I’ll get someone to pick me up, okay?” She takes a breath. “It’s fine, you can leave, Chris.” Clover looks less irritated and more exhausted from just yelling itself.</p>\n<p><strong>Ch:</strong> “I told you I forgot, okay? I hadn’t slept in 2 nights! I wasn’t mad at him, I just wanted him to know there were people trying to sleep - I’m sorry, okay?” For the first time we see his face, in a close up lit by the light on within the opened car. Chris has curly brown hair and is wearing thick glasses. He’s upset too, his face is tense and his lips pursed as his starts to say something but stops before the first word. He tries again, his eyes no longer looking at Clover in the headlights. He anxiously looks up at the streetlight and behind him and shifts uncomfortably in his jacket. “Clover, I’m really sorry it was late and I got worked up trying to stay cool about it all. I should have talked to you about it before. I...”</p>\n<p><strong>C:</strong> “Why is it that only now do you apologize for this kind of stuff? You wait until the last minute to say something - which is usually insulting and insensitive - and then only after I get upset and talk to you about it do you even consider you were wrong. You weren’t like this before, Chris.” She says the last line more quietly.</p>\n<p><strong>Ch:</strong> “I’m tired of this, you aren’t always right Clover. There are somethings that normal people can’t be expected to deal with. I can’t be sensitive to everything you are or I’d be living your life! I have my own life and some things bother me - like they would bother everyone else! I honestly don’t care that you have your own world, just don’t bring me into it. Don’t try to make everyone feel like you, ok? That’s the problem.” Clover looks up in deep odium. Her dark brown eyes are sunken in dark pools around her eyes. Her eyebrows furrow and she tucks her long brown hair behind her ears with a sigh. She stares up at the night sky. Even more tiredly, Clover responds.</p>\n<p><strong>C:</strong> “Look, Chris, I don’t know. I don’t know at what point something becomes disrespectful. But I know that there are other people living different lives. You and I are great examples of this. There’s been just enough tolerance between us for it to work so far. And I’ve never tried to make you feel the same way as me. I can’t imagine I’ve asked for a lot of sympathy, since it’s usually met with some long explanation about how it’s “unrealistic” for me to be feeling this way! But it’s just not worth arguing at some point if we can’t even relate to the events happening in our lives.” Clover looks back up at the sky for relief. “Yeah, know what, you might be right - I feel like I’m up on the moon sometimes and you’re on the total opposite side of Earth. And we’re always rotating and I’m always walking around trying to figure where the hell you are and you’re just sitting, sitting there watching me as you swing around and pass me again and again.”</p>\n<p><strong>Ch:</strong> “What can I do, Clover?” Chris hold his hands in the air, waiting for Clover to respond.</p>\n<p>Clover turns her back to the car and Chris and her face quickly tightens up and tears begin in well up in her eyes. She starts to pace away from the car and move across the parking lot, the light behind her now. The height of the shot elevates slowly from her shuffling sneakers, her tense hands gripping her phone, onto her face, still crying. Over her shoulder, we can see the blurry figure of a person leaning out from the open door of the driver’s seat of the sedan. The sound of Chris’s voice fades and echoes across the parking lot, we can still hear him yelling out for her as she begins to run. The sedan cranks on and drives away. Clover’s runs out of the light from parking lot. She pulls out her phone and turns on the flashlight. Both the light from the screen and from the flash illuminate her. Clover looks up from her phone and the shot cuts to a forest at the edge of the lot. She tops walking for a moment to stare up at it in awe. The sounds of the scene and a low drone rises up as if emanating from the forest itself, getting fuller and loud. As it does, Diane’s response returns in the darkness and the drone dissipates in an instant.</p>\n<h3 id=\"office-1\" tabindex=\"-1\">Office <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#office-1\">#</a></h3>\n<p><strong>Diane:</strong> “So, what are those worlds like? Could you explain them to me a little bit? Are they like daydreams?” The shot returns to Diane’s office, where the conversation has continued.</p>\n<p><strong>Clover:</strong> “Kind of, yeah, they’re drawn from the past, or at least in the way I’m reminded of it.” She pauses and it cuts back to a focus on Clover sitting with Diane. “I like to think about it as salvaging. Scraps of memories that, just, stick out for one reason or another. Like, last night I was writing about a memory I had as a kid. I remember one morning watching my mom roll the trash can up the driveway; I was looking from the window of my parents’ bedroom. I must have been 5 or 6.” As Clover’s speaking, the angle rotates from her profile towards the window, focusing on a wisteria tree, climbing the edge of the office outside. It focuses on a bundle of its soft pink flowers hovering in front of the window. Clover continues with the focus now on the tree. “And I remember how I felt watching my mom. She had no idea I was watching her, and the event was so mundane, yet, it felt so weird at the time. Now I probably would call it something like being “contemplative”, which I wasn’t really ever. It was odd, it was like, the realization that my mom existed, as a person, in a world without me. Of course, I must have known that even then, but I had just never seen it so closely” Clover smiles as she finishes her story, Diane looks down at the pad in her hands and quickly takes one note.</p>\n<p><strong>D:</strong> “Huh, well, that’s a lot deeper than I was expecting. I don’t know why you’re a math major!” She giggles. “So how does that exactly make a world? Are you reminiscing or -”</p>\n<p><strong>C:</strong> “Yeah, I mean a bit? It’s hard not to when writing stories. I mean, I can’t not write from experience, right?” She pauses to think “What do you think? Should I stop? I think they give me a chance to reconnect? Or explore the what I remember of the past, now, being older and all.</p>\n<p><strong>D:</strong> “No, no, not at all. I think that’s wonderful, and therapeutic I’m sure! I was just curious how you felt about them and whether they felt good for you. That’s all.”</p>\n<p><strong>C:</strong> ”Yeah, I think - it’s just an exercise that I can tell myself is productive.” Clover leans her head on the arms of the sofa she’s sitting on. “Do you think about old memories often?”</p>\n<h3 id=\"suburbs\" tabindex=\"-1\">Suburbs <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#suburbs\">#</a></h3>\n<p>The scene cuts to Clover squeezing through the trimmed wall of a hedge. It’s midnight. Crickets and cicadas buzz loudly around the shuffle of Clover’s figure through snapping and whipping twigs that trail her. She walks onto a street of a neighborhood. She left the yard of a large brick house through a wall of vegetation that surrounds it.</p>\n<p><strong>C:</strong> “Come on!” She whispers loudly to the fern, and a second body bursts from the trimmed hedge. “Come on! Let’s go!”</p>\n<p><strong>Nico:</strong> “I’m coming! Ow, ow, was this the best route?” She frees her leg from the hedge and jogs over to Clover. “And more importantly, do you even know where’s the lake from here?”</p>\n<p><strong>C:</strong> “Well, I got here, didn’t I? I can figure my way over there too, or we’ll just be wandering the night till morning then.” She grins wide and tucks her loose brown hair, already in a bun, behind her ears. Clover is excited, unstressed and unrestrained. She pulls some earplugs from her pocket. “You brought yours too right? Don’t tell me you forgot?”</p>\n<p><strong>N:</strong> “I have it, I have it! she awkwardly pulls her iPod out from her back pocket and shakes it in front of Clover. “See, I don’t forget. I even put the songs you sent me on it. And, let me tell you, that took me hours, sooo-” Nico’s eyes float to the tree branches overhead as she trails off.</p>\n<p><strong>C:</strong> “Good! Now at least you have some good music on that thing” She grins and leans in with her headphones already in. “Let’s go!”</p>\n<p>At that moment, Clover puts in her headphones, quickly fidgets with her iPod and begins to dance in the middle of the street. Clover’s dance is goofy and a bit uncoordinated. She bobs and swings her arms around across the street, keeping her eyes and grin on Nico as she too, though much more slowly, set her iPod up to go as well. Clover dances over to Nico after a moment.</p>\n<p><strong>C:</strong> ”Oh, oh, okay so, open the playlist I sent you - I almost forgot - here, let’s me see, yeah, and there.” Clover smoothly works the iPod to the playlist, “Now... I’m going to press play at the same time as you and we should be in sync for as long as this playlist lasts!”</p>\n<p><strong>N:</strong> ”You mean you hadn’t put any music on yet?”</p>\n<p><strong>C:</strong> ”No! I was waiting for your slow ass!”</p>\n<p><strong>N:</strong> ”Then, what were you dancing to, weirdo?”</p>\n<p><strong>C:</strong> “You of course!” Nico smiles at this and shyly shakes her head. “Okay, ready yet?”</p>\n<p><strong>N:</strong> “How long is this playlist, again?”</p>\n<p><strong>C:</strong> “I think we’ll be good until 4 or 5am!”</p>\n<p><strong>N:</strong> “Oh my god,” Nico rolls her eyes. &quot;I hope your taste in music is as good as you said!”</p>\n<p>The camera swings to Clover who is already dancing down the street and it ascends upwards still keeping a focus on the street. Above the trees you can see the neighborhood fully enclosed within the branches of a suburban grove of oaks. The perspective gazes down at the street while moving forward, still hovering above the two bodies walking between the edges of the tree branches until the branches converge in a thicket and the shot fades to black.</p>\n<p>Nico and Clover are now center frame, walking along a road with fewer houses, street lights and trees, at the edge of the neighborhood. It’s much darker, with fewer street lamps along the street. Beside them is a plot of undeveloped land, where construction crews are in the middle of filling out the frames of 6 or 7 houses. The unfinished buildings stand like skeletons over the field. Clover points beyond the houses to a tree-line about half a mile away.</p>\n<p><strong>C:</strong> “Through there. I’m almost certain” She pulls out Nico’s left earbud to tell her.</p>\n<p><strong>N:</strong> “And you saw this pond where?</p>\n<p><strong>C:</strong> “ Online, I looked it up! It’s right on the other side of those trees! Someone said that the builders here wanted to buy that land but apparently someone privately owns that land and wouldn’t sell to them.”</p>\n<p><strong>N:</strong> “You sure about that?”</p>\n<p><strong>C:</strong> “Yeah, I mean, it was the only comment on Google. I’d say it’s about 50/50. Who knows.”</p>\n<p><strong>N:</strong> “That’s odd, maybe it’s a private fishing pond?”</p>\n<p><strong>C:</strong> “I’m not sure. But, whatever it is - I think there’s a dock in the middle. Did you bring something to swim in?” Clover smirks unevenly, waiting for Nico’s response.</p>\n<p><strong>N:</strong> “Oh please. Like I’m going to hop in a random pond in the middle at midnight.”</p>\n<p>Clover shrugs and keeps walking, now off-road, through the untrimmed yards of the new houses. Behind Clover, Nico is noticeably reserved, her arms are kept close to her heavy wool sweater and is hesitant, at first, to step off the road but eventually joins Clover. Clover, holds her hands up to the sky, swinging them back and forth as she’s singing along with current track on her headphones. Her voice sounds quiet next to the roar of insects that fills the field. Nico has caught up with Clover and both are now moving more cautiously as the tall, dense grass begins to hide their feet. They move in unison quietly, for a while. The sound of their steps is loud and make a satisfying crunch over dry reeds.</p>\n<p>Suddenly, Nico’s arms wrap Clover’s shoulders from behind, her head falling on a shoulder and they come to a stop before the tree-line. It’s playful and unexpected, and uncoordinated, causing the two to stumble forward and Clover’s earbuds to fall out. For the first time, the Clover’s bright countenance begins to sink as Nico’s hands come together over her chest. Clover takes a sharp breath in at Nico’s gesture. With the music gone, she starts reorient to the deafening sound around her and begins to look around at where she is as if unaware of how she got here. We stay here watching her and Nico for a while, who has retracted her arms to the tops of Clover’s shoulders and is also staring at the woods. As the growl grows, the scene cuts to outside Clover’s dorm, where she is still recovering from her sprint through the dorm.</p>\n<h3 id=\"home\" tabindex=\"-1\">Home <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#home\">#</a></h3>\n<p>Her breathing returns to normal. She begins to quickly walk towards the parking lot, where the sun has begun to set and gets into her car. From the inside of her old tan minivan, she pulls out her phone to dial and begins to drive out of the lot.</p>\n<p><strong>C:</strong> “Mom? You there? Is he alright? Is he okay? I’m driving over right now, okay?” She pauses to listen to a sharp voice through the phone “I know, I know you did. It’s okay. I’ll be there soon, okay? Ok? He’s going to be okay. I’m heading over. Don’t worry - yes, of course. I love you. Bye.”</p>\n<p>She turns on the emergency blinkers and speeds through the highway onto the offramp. Entering into a neighborhood, Clover swerves past stop signs and worried parents that watch front yards, some picking up the paper, rolling out the garbage bins, watering the yard, chatting, or holding back their kids from the accelerating vehicle. Crimson flashes of light begin to appear on her face as she finally slows down. Outside, theres a commotion around the front yard of a small single-story house. Police are trying to speak with an older woman who is sobbing on the brick steps of her house. Clover slams on the brakes in front of the lawn, opens her door and runs over, forgetting to close it in her haste.</p>\n<p><strong>C:</strong> “Mom, mom, what happened?!” Clover is holding her mother, Samantha, on the steps. She brushes Samantha’s gray hair from her face. Clover’s startled by the number of wrinkles under her mom’s eyes and hugs her close. Tears have collected in those wrinkles. “What happened, tell me. Did he have another episode.”\nSamantha: “He - he ran away, Clover. I tried to hold him and he ran away from me.” Clover notices the sanguine bruises on her face and blood on the edges on her mouth.</p>\n<p><strong>C:</strong> “Mom... what happened? Are you okay? Did he hit you? Why is there blood on your face?”</p>\n<p><strong>S:</strong> “Clover, clover, it’s alright, it’s okay you know he gets that way, it was my mistake to try and hold him - he was trying to escape. He was screaming and thrashing around in his room. I went in to see what was happening”</p>\n<p><strong>C:</strong> “No, mom, he’s never done that before. Did he run away? Where is he?”</p>\n<p><strong>S:</strong> “He - ran out through the back. I couldn’t chase him - he knocked me down! I’m sorry, Clover, I’m sorry.”</p>\n<p><strong>C:</strong> “Mom, its okay, it’s alright, we’re going to find him don’t worry. I’ll find him.”</p>\n<p>Clover turns around to two police officers, watching the scene and chatting on radio. Noticing that Clover has finished talking, a tall man with crew cut brown hair walks over.</p>\n<p><strong>P1:</strong> “Are you the sister of Cecil Dawning?”</p>\n<p><strong>C:</strong> “Yes”</p>\n<p><strong>P1:</strong> “Okay good, as your mom might have told you, your brother ran off about an hour ago from the back of your home, south.” The officer points through the house, over towards the woods that lie behind Clover’s neighborhood. “We are going to need to ask you some questions about your brother if you...” The officer’s voice trails off into the distance and the sound of crickets and cicadas rise.</p>\n<p>Clover is struck with fear. Her facial expression has changed as the shot returns to her face. Her eyes that were once looking in the direction of his finger, now seem to be looking nowhere, absent of focus. Her figure is outlined with an aura of flashing red and blue from the police car in the lawn.</p>\n<p>Suddenly, the shot seamlessly changes to night as she turns around towards the lawn. Everyone is gone, the police, their car, Samantha are all absent as clover continues to star into the distance. Her face is now lit by the moon overhead and its cool blue light covers the rest of the neighborhood. All colors are now in neutral shades of blue. The noise of the crickets and cicadas continue.</p>\n<p>Without notice of the change, Clover opens the screen door in front of her and begins to walk through her house. The house is pitch black. From over Clover’s shoulders, the only light that is shining through is that of the back door. It beams light across the living room where a cabinet has been toppled, spilling books and shattered ceramic and glass across the carpet floor. The sound of Diane’s voice breaks through the silence. It speaks over the current scene as the visuals continue.</p>\n<p><strong>D:</strong> “To answer your question Clover, I do think about my memories. Often, in the same way as you. Only I’ve have about twice your life to think about, twice as many decisions with slip-ups and joys. But you do get better at it, at least you care less about your frequent mistakes and more about the disturbances you’re convinced are unavoidable.”\nClover looks down at the mess but is uninterested having fallen under some sort of delirium. Her gaze returns to the door, and continues to walk through the living room.</p>\n<p><strong>C:</strong> “But do you ever fear that you’ll forget, one day? What will happen when there are no more clues to your memories. That sudden rush of the experiences you had forgotten when you hear an old voice, or see a face, are harder to recall, and even more dependent on the pure chance of bumping into those people or places I spent time with.”</p>\n<p>By the back door is another room, whose door is open. Clover walks up to the backdoor and looks left, into the open bedroom. There, in the center of the room is a bare mattress made visible by the light shining through a small window across from her, covered partially with the torn sheets. A mangled mess of clothes and trash are pilled in corners of the bedroom; nothing is in order. The walls are bare and speckled with small punctures. Besides the bed, there’s a dresser and old chair that’s been overturn. Clover looks over the disarray and takes a calm inhale. Now looking out from the ground of the bedroom, she turns away from the doorway and opens the backdoor to outside.</p>\n<p><strong>C:</strong> “I mean where do they go? Where is it all? The high-school dances and snacks my mom would bring me Saturday afternoons? Those cartoons I watched and all the hikes I spent sprinting up the mountain like a goat, falling and bleeding all over the car? Biking until the late evening wind sent me home, missing dinner, and staying up late with an assignment and never learning to plan ahead. Taking a nervous hand like I would a pillow at night. I mean, they’re all there. But their experience... it wasn’t poetic.” She trails off with a sigh.</p>\n<p>Looking outside her house, she’s confronted with a tall thicket of trees 20 feet from her. She turns back towards the inside of the house, where her turn returns the scene to daylight with its sound muffled, her mother’s cries and the police officer calling her name from the other side of the house are visible, asking her to come back to answer questions, and telling Clover they are already searching for her brother. Those sounds are muffled, and don’t puncture the sound of night. Interrupting the commotion at hand, she returns towards the forest, again transitioning to night, and bolts into the woods. The pitter-patter of crunching leaves can be heard even after the darkness of the woods obscures her.</p>\n<h3 id=\"forest\" tabindex=\"-1\">Forest <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#forest\">#</a></h3>\n<p>Cut to black. The sound of steps in on the forest floor are all that can be heard. Clover slowly becomes visible in the light of the moon that’s broken through the tree tops. The shot follows her from the front, the sound of footsteps crunching through the fallen leaves. The steps quietly multiply, and soon Clover’s steps aren’t the only ones in the forest. She stops for a moment and the other footsteps stop a half-second after her. Clover looks fear-struck, and begins to turn around but stops, her head bent at her shoulder. The air is cold. A close up of her lips show her breaths, warm clouds, entering and exiting through her mouth. Carefully, she returns her gaze ahead. She continues to walk forward and the sound of footsteps are now her own. Soon, Clover comes to a clearing. Rising from behind her back, a large, dark pond stretches before her. She walks towards the body of water. The scintillating shape of the moon, hovers over the surface of the pond. Towards the edge of the pond that Clover’s facing, there’s an old wooden dock. As she moves towards the dock, she begins to make out the outline of a person sitting at its end. She walks over to the edge and sits down, feet dangling over the edge. We watch the two from the beginning of the dock. Clover rests her head on the shoulder of the person she still hasn’t looked at yet. A close shot of Clover, whose face is lit by the pond’s clean reflection of the moonlight.</p>\n<p><strong>C:</strong> “Was I over-detailed in their descriptions?” She starts wearily, “How else was I supposed to remember? Build? Start again? Turn backwards? for the sake of a road ahead.” Her hands thrusting towards the pond air. “And now, they’re getting worn out and indiscernible - from the daily rotation of hormones and maddening routines, like a spool of tape. We’ve just dunked it in developer and pulled it out for a fresh exposure.” She inhales and exhales and a smooth breeze descends from the treetops over the water, “I wish I wasn’t still spending the mornings thawing out from sleep. Spending however many minutes remembering, again, where I was when I fell asleep and that my eyes are now open. I sit up, silence slowly rubs my back and I look over to grimace at all the things I’ve surrounded myself with.”</p>\n<p>Clover lays back against the dock, her eyes looking up at the stars. An owl starts to coo across the open air.</p>\n<p><strong>C:</strong> “But, I’ve been thinking, too, of frightening thoughts, with a gravity of their own. Where, if you’re careful and patient, you might be able to watch existence orbit around their place. They roam around the sky, these massive harmonies, passing through the day silently - undetected. Until, at last, someone looks up. They look up and there it is, the atmosphere stretched past the edges of your eyes. A sky, filled with dancers, the phantoms of new memory, all the days I’ve been searching for that I’d thought I lost. The planet starts to slip from the upturned head, maybe mine. And they begin dancing too! Oh! all those afternoons sought for their nothingness; our simple want that was patient past its years. Instead, we solemnly watched it pass, mysteriously, like a sun setting out of tune.” She smiles at the thought and stops. The figure that was sitting next to her is gone.</p>\n<p>Looking down unto the dock, the shot elevates until the dock, the lake and finally the forests come into view. The perspective continues to rise and the scene fades to black.</p>\n<h3 id=\"end\" tabindex=\"-1\">End <a class=\"header-anchor\" href=\"https://jackson.gd/blog/2018-11-02-43-rpm/#end\">#</a></h3>\n",
			"date_published": "2018-11-02T00:00:00Z"
		}
		
	]
}
